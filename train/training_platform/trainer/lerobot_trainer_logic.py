# training_platform/trainer/lerobot_trainer_logic.py (æ¨¡å—åŒ–ç‰ˆæœ¬)

import logging
from pathlib import Path
from pprint import pformat
from typing import Callable, Dict, Any, Tuple

import torch
from torch.amp.grad_scaler import GradScaler
import draccus
import os
from training_platform.configs.settings import settings

# é…ç½®æ—¥å¿—
# åˆ›å»ºæ—¥å¿—ç›®å½•
import os
log_dir = os.path.join(os.getcwd(), 'logs')
os.makedirs(log_dir, exist_ok=True)

logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
    handlers=[
        logging.StreamHandler(),
        logging.FileHandler(os.path.join(log_dir, 'training.log'))
    ]
)

# å¯¼å…¥æ‰€æœ‰ lerobot çš„ä¾èµ–
from lerobot.common.datasets.factory import make_dataset
from lerobot.common.datasets.sampler import EpisodeAwareSampler
from lerobot.common.datasets.utils import cycle
from lerobot.common.policies.factory import make_policy
from lerobot.common.optim.factory import make_optimizer_and_scheduler
from lerobot.common.utils.logging_utils import AverageMeter, MetricsTracker
from lerobot.common.utils.train_utils import (
    get_step_checkpoint_dir,
    load_training_state,
    save_checkpoint,
    async_save_checkpoint,
    update_last_checkpoint,
)
from lerobot.common.utils.utils import get_safe_torch_device
from lerobot.scripts.train import update_policy
from lerobot.configs.train import TrainPipelineConfig

# åœ¨æ¨¡å—åŠ è½½æ—¶ï¼Œç¡®ä¿æ‰€æœ‰ lerobot çš„æ’ä»¶éƒ½è¢«æ³¨å†Œ
import lerobot.common.envs.factory
import lerobot.common.policies.factory


def prepare_config(
    base_config_path: str,
    user_override_config: Dict[str, Any],
    run_dir: str,
    task_id: int, 
    start_step: int,
    end_step: int,
) -> TrainPipelineConfig:
    """
    ç¬¬ä¸€é˜¶æ®µï¼šåŠ è½½ã€åˆå¹¶å¹¶å‡†å¤‡æœ€ç»ˆçš„è®­ç»ƒé…ç½®å¯¹è±¡ã€‚
    """
    logging.info(f"Loading base config from: {base_config_path}")
    logging.info(f"Applying user override config: {user_override_config}")
    
    # æ„å»ºå‘½ä»¤è¡Œå‚æ•°åˆ—è¡¨
    overrides = []
    
    # æ·»åŠ resumeå‚æ•°ï¼ˆå¦‚æœä»checkpointæ¢å¤ï¼‰
    if start_step > 0:
        overrides.append("--resume=true")
    
    # æ·»åŠ config_pathå‚æ•°ï¼Œç›´æ¥æŒ‡å‘çœŸå®å­˜åœ¨çš„é…ç½®æ–‡ä»¶
    overrides.append(f"--config_path={base_config_path}")
    
    # æ·»åŠ å…¶ä»–ç”¨æˆ·è¦†ç›–çš„é…ç½®
    for key, value in user_override_config.items():
        if key == "resume":  # è·³è¿‡resumeï¼Œå› ä¸ºæˆ‘ä»¬å·²ç»å¤„ç†äº†
            continue
        if isinstance(value, dict):
            # å¯¹äºåµŒå¥—å­—å…¸ï¼Œæˆ‘ä»¬éœ€è¦å°†å…¶å±•å¹³ä¸ºç‚¹åˆ†éš”çš„æ ¼å¼
            for nested_key, nested_value in value.items():
                if isinstance(nested_value, dict):
                    # å¤„ç†æ›´æ·±å±‚çš„åµŒå¥—
                    for deep_key, deep_value in nested_value.items():
                        overrides.append(f"--{key}.{nested_key}.{deep_key}={deep_value}")
                else:
                    overrides.append(f"--{key}.{nested_key}={nested_value}")
        else:
            overrides.append(f"--{key}={value}")
    
    # å¦‚æœæ•°æ®é›†å·²ç»ä¸‹è½½åˆ°æœ¬åœ°ï¼Œåˆ™è¦†ç›– repo_id
    local_dataset_unpacked_path = Path(run_dir) / "dataset"
    local_dataset_repo_id = os.path.join(str(task_id), "dataset")
    print(f"Local dataset path: {local_dataset_unpacked_path}")
    if local_dataset_unpacked_path.exists():
        print(f"Local dataset found at: {local_dataset_unpacked_path}")
        overrides.append(f"--dataset.root={local_dataset_unpacked_path}")
        overrides.append(f"--dataset.repo_id={local_dataset_repo_id}")
    
    logging.info(f"Command line overrides: {overrides}")
    
    # ä½¿ç”¨draccusè§£æé…ç½®ï¼Œç›´æ¥ä¼ å…¥å‘½ä»¤è¡Œå‚æ•°
    cfg: TrainPipelineConfig = draccus.parse(
        config_class=TrainPipelineConfig,
        args=overrides,
    )
    
    # æ‰‹åŠ¨è®¾ç½®config_pathå±æ€§ï¼Œç¡®ä¿validate()æ–¹æ³•èƒ½æ­£ç¡®è®¿é—®
    if start_step > 0:
        # å½“resume=Trueæ—¶ï¼Œæ‰‹åŠ¨è®¾ç½®config_path
        # æˆ‘ä»¬éœ€è¦ä¿®æ”¹validate()æ–¹æ³•çš„é€»è¾‘ï¼Œè®©å®ƒèƒ½æ­£ç¡®è·å–config_path
        import sys
        # ä¸´æ—¶ä¿®æ”¹sys.argvï¼Œè®©parser.parse_argèƒ½æ­£ç¡®å·¥ä½œ
        original_argv = sys.argv.copy()
        sys.argv = [sys.argv[0]] + overrides
        
        try:
            # ç°åœ¨validate()åº”è¯¥èƒ½æ­£ç¡®è·å–config_path
            cfg.validate()
        finally:
            # æ¢å¤åŸå§‹çš„sys.argv
            sys.argv = original_argv
    else:
        # éæ¢å¤è®­ç»ƒï¼Œç›´æ¥è°ƒç”¨validate
        cfg.validate()

    logging.info("Successfully created final training configuration.")
    logging.info(pformat(cfg.to_dict()))
    return cfg

# TODO resume the previous training state, reserve and restore the training state --resume
def initialize_training_objects(
    cfg: TrainPipelineConfig,
    device: torch.device,
    start_step: int,
) -> Tuple:
    """
    ç¬¬äºŒé˜¶æ®µï¼šæ ¹æ®é…ç½®åˆå§‹åŒ–æ‰€æœ‰è®­ç»ƒæ‰€éœ€çš„å¯¹è±¡ã€‚
    """
    logging.info("Creating dataset...")
    dataset = make_dataset(cfg)

    logging.info("Creating policy...")
    policy = make_policy(cfg=cfg.policy, ds_meta=dataset.meta)
    policy.to(device)

    logging.info("Creating optimizer and scheduler...")
    optimizer, lr_scheduler = make_optimizer_and_scheduler(cfg, policy)
    grad_scaler = GradScaler(device.type, enabled=cfg.policy.use_amp)

    # åŠ è½½ Checkpoint (å¦‚æœæ–­ç‚¹ç»­ç»ƒ)
    if start_step > 0:
        checkpoint_path = Path(cfg.output_dir) / "checkpoints" / "last"
        if checkpoint_path.is_symlink() or checkpoint_path.exists():
            logging.info(f"Resuming training from checkpoint: {checkpoint_path.resolve()}")
            # ä¿®å¤ï¼šç§»é™¤ä¸å­˜åœ¨çš„å‚æ•°
            loaded_step, optimizer, lr_scheduler = load_training_state(
                checkpoint_path, optimizer, lr_scheduler
            )
            logging.info(f"Checkpoint loaded, was at step {loaded_step}. Starting from {start_step}.")
        else:
            logging.warning(f"Expected a checkpoint at {checkpoint_path} for resuming, but not found.")

    return dataset, policy, optimizer, lr_scheduler, grad_scaler


async def execute_training_loop(
    cfg: TrainPipelineConfig,
    device: torch.device,
    start_step: int,
    end_step: int,
    training_objects: Tuple,
    log_callback: Callable,
    save_callback: Callable,
):
    """
    ç¬¬ä¸‰é˜¶æ®µï¼šæ‰§è¡Œæ ¸å¿ƒçš„è®­ç»ƒå¾ªç¯ã€‚
    """
    dataset, policy, optimizer, lr_scheduler, grad_scaler = training_objects
    
    # Dataloader å’Œè®­ç»ƒå‡†å¤‡
    if hasattr(cfg.policy, "drop_n_last_frames"):
        sampler = EpisodeAwareSampler(
            dataset.episode_data_index, drop_n_last_frames=cfg.policy.drop_n_last_frames, shuffle=True
        )
        shuffle = False
    else:
        sampler = None
        shuffle = True
        
    dataloader = torch.utils.data.DataLoader(
        dataset, 
        num_workers=cfg.num_workers,
        batch_size=cfg.batch_size, 
        shuffle=shuffle, 
        sampler=sampler, 
        pin_memory=device.type != "cpu", 
        drop_last=False
    )
    dl_iter = cycle(dataloader)
    
    if start_step > 0:
        logging.info(f"Fast-forwarding dataloader by {start_step} steps...")
        for _ in range(start_step):
            next(dl_iter)

    policy.train()

    train_metrics = {
        "loss": AverageMeter("loss", ":.3f"), "grad_norm": AverageMeter("grdn", ":.3f"), "lr": AverageMeter("lr", ":0.1e"),
        "update_s": AverageMeter("updt_s", ":.3f"), "dataloading_s": AverageMeter("data_s", ":.3f"),
    }
    train_tracker = MetricsTracker(
        cfg.batch_size, dataset.num_frames, dataset.num_episodes, train_metrics, initial_step=start_step
    )

    logging.info(f"Entering training loop from step {start_step} to {end_step}...")
    for step in range(start_step, end_step):
        batch = next(dl_iter)

        for key in batch:
            if isinstance(batch[key], torch.Tensor):
                batch[key] = batch[key].to(device, non_blocking=True)

        train_tracker, output_dict = update_policy(
            train_tracker,
            policy,
            batch,
            optimizer,
            cfg.optimizer.grad_clip_norm,
            grad_scaler=grad_scaler,
            lr_scheduler=lr_scheduler,
            use_amp=cfg.policy.use_amp,
        )
        current_step = step + 1

        is_log_step = cfg.log_freq > 0 and current_step % cfg.log_freq == 0
        is_periodic_saving_step = current_step % cfg.save_freq == 0

        if is_log_step:
            log_callback(current_step, train_tracker.to_dict())
            train_tracker.reset_averages() 

        try:
            if cfg.save_checkpoint and is_periodic_saving_step:
                logging.info(f"Saving checkpoint at step {current_step}")
                checkpoint_dir = get_step_checkpoint_dir(cfg.output_dir, cfg.steps, current_step)
                logging.info(f"Saving checkpoint to: {checkpoint_dir}")
                await async_save_checkpoint(checkpoint_dir, current_step, cfg, policy, optimizer, lr_scheduler)
                update_last_checkpoint(checkpoint_dir)
                
                save_callback(current_step, str(checkpoint_dir))
        except Exception as e:
            logging.error(f"CRITICAL: `save_checkpoint` failed at step {current_step} with error: {e}", exc_info=True)
            raise e
    
    # ----- å¾ªç¯ç»“æŸåï¼Œå¼ºåˆ¶ä¿å­˜å½“å‰åˆ†ç‰‡çš„æœ€ç»ˆçŠ¶æ€ -----
    logging.info(f"Finished training slice loop at step {end_step}. Performing final save for this slice.")
    
    # æ£€æŸ¥è¿™ä¸€æ­¥æ˜¯å¦å·²ç»åœ¨å¾ªç¯çš„æœ€åä¸€æ¬¡è¿­ä»£ä¸­ä¿å­˜è¿‡äº†
    last_step_already_saved = (end_step % cfg.save_freq == 0)

    try:
        if cfg.save_checkpoint and not last_step_already_saved:
            logging.info(f"Saving final state of the slice at step {end_step}")
            checkpoint_dir = get_step_checkpoint_dir(cfg.output_dir, cfg.steps, end_step)
            logging.info(f"Saving final checkpoint to: {checkpoint_dir}")
            await save_checkpoint(checkpoint_dir, end_step, cfg, policy, optimizer, lr_scheduler)
            update_last_checkpoint(checkpoint_dir)
            save_callback(end_step, str(checkpoint_dir))
        elif last_step_already_saved:
            logging.info(f"Step {end_step} was already saved as a periodic checkpoint. Skipping duplicate save.")
    except Exception as e:
        logging.error(f"CRITICAL: `save_checkpoint` failed at step {end_step} with error: {e}", exc_info=True)
        raise e

    logging.info(f"Finished training slice. Final step for this slice: {end_step}")
    return end_step


async def run_lerobot_training(
    base_config_path: str,
    user_override_config: Dict[str, Any],
    run_dir: str,
    task_id: int,
    start_step: int,
    end_step: int,
    log_callback: Callable,
    save_callback: Callable,
):
    """
    ä¸»å…¥å£å‡½æ•°ï¼Œç¼–æ’æ•´ä¸ªè®­ç»ƒæµç¨‹ã€‚
    """
    # é˜¶æ®µä¸€ï¼šå‡†å¤‡é…ç½®
    
    cfg = prepare_config(base_config_path, user_override_config, run_dir, task_id, start_step, end_step)

    print(f"base_config_path: {base_config_path}")
    print(f"user_override_config: {user_override_config}")
    # print(f"cfg: {cfg}")
    print(f"run_dir: {run_dir}")
    # è®¾ç½®è®¾å¤‡
    device = get_safe_torch_device(cfg.policy.device, log=True)
    torch.backends.cudnn.benchmark = True
    torch.backends.cuda.matmul.allow_tf32 = True

        # æ·»åŠ GPUç›‘æ§ä»£ç 
    print(f"ğŸ” GPUç›‘æ§ä¿¡æ¯:")
    print(f"   - å®é™…ä½¿ç”¨çš„è®¾å¤‡: {device}")
    print(f"   - CUDAå¯ç”¨: {torch.cuda.is_available()}")
    if torch.cuda.is_available():
        print(f"   - GPUæ•°é‡: {torch.cuda.device_count()}")
        print(f"   - å½“å‰GPU: {torch.cuda.current_device()}")
        print(f"   - GPUåç§°: {torch.cuda.get_device_name()}")
        print(f"   - GPUå†…å­˜: {torch.cuda.get_device_properties(0).total_memory / 1024**3:.1f} GB")
    

    # é˜¶æ®µäºŒï¼šåˆå§‹åŒ–æ‰€æœ‰å¯¹è±¡
    training_objects = initialize_training_objects(cfg, device, start_step)
    
    # é˜¶æ®µä¸‰ï¼šæ‰§è¡Œè®­ç»ƒå¾ªç¯
    final_step = await execute_training_loop(
        cfg, device, start_step, end_step,
        training_objects, log_callback, save_callback
    )

    return final_step           
