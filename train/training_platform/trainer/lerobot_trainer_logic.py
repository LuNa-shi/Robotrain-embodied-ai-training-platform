# training_platform/trainer/lerobot_trainer_logic.py (æ¨¡å—åŒ–ç‰ˆæœ¬)

import logging
from pathlib import Path
from pprint import pformat
from typing import Callable, Dict, Any, Tuple

import torch
import torch.distributed as dist
from torch.nn.parallel import DistributedDataParallel as DDP
from torch.utils.data.distributed import DistributedSampler
from torch.amp.grad_scaler import GradScaler
import draccus
import os
from training_platform.configs.settings import settings
from training_platform.configs.settings import settings
from training_platform.common.rabbitmq_utils import init_rabbitmq, send_log_message

# é…ç½®æ—¥å¿—
# åˆ›å»ºæ—¥å¿—ç›®å½•
import os
log_dir = os.path.join(os.getcwd(), 'logs')
os.makedirs(log_dir, exist_ok=True)

logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
    handlers=[
        logging.StreamHandler(),
        logging.FileHandler(os.path.join(log_dir, 'training.log'))
    ]
)

# å¯¼å…¥æ‰€æœ‰ lerobot çš„ä¾èµ–
from lerobot.common.datasets.factory import make_dataset
from lerobot.common.datasets.sampler import EpisodeAwareSampler
from lerobot.common.datasets.utils import cycle
from lerobot.common.policies.factory import make_policy
from lerobot.common.optim.factory import make_optimizer_and_scheduler
from lerobot.common.utils.logging_utils import AverageMeter, MetricsTracker
from lerobot.common.utils.train_utils import (
    get_step_checkpoint_dir,
    load_training_state,
    save_checkpoint,
    async_save_checkpoint,
    update_last_checkpoint,
)
from lerobot.common.utils.utils import get_safe_torch_device
from lerobot.scripts.train import update_policy
from lerobot.configs.train import TrainPipelineConfig

# åœ¨æ¨¡å—åŠ è½½æ—¶ï¼Œç¡®ä¿æ‰€æœ‰ lerobot çš„æ’ä»¶éƒ½è¢«æ³¨å†Œ
import lerobot.common.envs.factory
import lerobot.common.policies.factory


def prepare_config(
    base_config_path: str,
    user_override_config: Dict[str, Any],
    run_dir: str,
    task_id: int, 
    start_step: int,
    end_step: int,
    is_distributed: bool = False,
) -> TrainPipelineConfig:
    """
    ç¬¬ä¸€é˜¶æ®µï¼šåŠ è½½ã€åˆå¹¶å¹¶å‡†å¤‡æœ€ç»ˆçš„è®­ç»ƒé…ç½®å¯¹è±¡ã€‚
    """
    logging.info(f"Loading base config from: {base_config_path}")
    logging.info(f"Applying user override config: {user_override_config}")
    
    # æ„å»ºå‘½ä»¤è¡Œå‚æ•°åˆ—è¡¨
    overrides = []
    
    # æ·»åŠ resumeå‚æ•°ï¼ˆå¦‚æœä»checkpointæ¢å¤ï¼‰
    if start_step > 0:
        overrides.append("--resume=true")
    
    # æ·»åŠ config_pathå‚æ•°ï¼Œç›´æ¥æŒ‡å‘çœŸå®å­˜åœ¨çš„é…ç½®æ–‡ä»¶
    overrides.append(f"--config_path={base_config_path}")
    
    # æ·»åŠ å…¶ä»–ç”¨æˆ·è¦†ç›–çš„é…ç½®
    for key, value in user_override_config.items():
        if key == "resume":  # è·³è¿‡resumeï¼Œå› ä¸ºæˆ‘ä»¬å·²ç»å¤„ç†äº†
            continue
        if isinstance(value, dict):
            # å¯¹äºåµŒå¥—å­—å…¸ï¼Œæˆ‘ä»¬éœ€è¦å°†å…¶å±•å¹³ä¸ºç‚¹åˆ†éš”çš„æ ¼å¼
            for nested_key, nested_value in value.items():
                if isinstance(nested_value, dict):
                    # å¤„ç†æ›´æ·±å±‚çš„åµŒå¥—
                    for deep_key, deep_value in nested_value.items():
                        overrides.append(f"--{key}.{nested_key}.{deep_key}={deep_value}")
                else:
                    overrides.append(f"--{key}.{nested_key}={nested_value}")
        else:
            overrides.append(f"--{key}={value}")
    
    # å¦‚æœæ•°æ®é›†å·²ç»ä¸‹è½½åˆ°æœ¬åœ°ï¼Œåˆ™è¦†ç›– repo_id
    local_dataset_unpacked_path = Path(run_dir) / "dataset"
    local_dataset_repo_id = os.path.join(str(task_id), "dataset")
    print(f"Local dataset path: {local_dataset_unpacked_path}")
    if local_dataset_unpacked_path.exists():
        print(f"Local dataset found at: {local_dataset_unpacked_path}")
        overrides.append(f"--dataset.root={local_dataset_unpacked_path}")
        overrides.append(f"--dataset.repo_id={local_dataset_repo_id}")
    
    logging.info(f"Command line overrides: {overrides}")
    
    # ä½¿ç”¨draccusè§£æé…ç½®ï¼Œç›´æ¥ä¼ å…¥å‘½ä»¤è¡Œå‚æ•°
    cfg: TrainPipelineConfig = draccus.parse(
        config_class=TrainPipelineConfig,
        args=overrides,
    )

    if is_distributed:
        world_size = dist.get_world_size()
        # å‡è®¾åŸå§‹batch_sizeæ˜¯å…¨å±€batch_sizeï¼Œå°†å…¶åˆ†é…åˆ°å„ä¸ªGPUä¸Š
        if cfg.batch_size % world_size != 0:
            logging.warning(f"Global batch size {cfg.batch_size} is not divisible by world size {world_size}. This may lead to uneven batches.")
        cfg.batch_size = cfg.batch_size // world_size
        logging.info(f"Distributed mode: World size = {world_size}, batch size per GPU set to {cfg.batch_size}")
    
    # æ‰‹åŠ¨è®¾ç½®config_pathå±æ€§ï¼Œç¡®ä¿validate()æ–¹æ³•èƒ½æ­£ç¡®è®¿é—®
    if start_step > 0:
        # å½“resume=Trueæ—¶ï¼Œæ‰‹åŠ¨è®¾ç½®config_path
        # æˆ‘ä»¬éœ€è¦ä¿®æ”¹validate()æ–¹æ³•çš„é€»è¾‘ï¼Œè®©å®ƒèƒ½æ­£ç¡®è·å–config_path
        import sys
        # ä¸´æ—¶ä¿®æ”¹sys.argvï¼Œè®©parser.parse_argèƒ½æ­£ç¡®å·¥ä½œ
        original_argv = sys.argv.copy()
        sys.argv = [sys.argv[0]] + overrides
        
        try:
            # ç°åœ¨validate()åº”è¯¥èƒ½æ­£ç¡®è·å–config_path
            cfg.validate()
        finally:
            # æ¢å¤åŸå§‹çš„sys.argv
            sys.argv = original_argv
    else:
        # éæ¢å¤è®­ç»ƒï¼Œç›´æ¥è°ƒç”¨validate
        cfg.validate()

    logging.info("Successfully created final training configuration.")
    logging.info(pformat(cfg.to_dict()))
    return cfg

# TODO resume the previous training state, reserve and restore the training state --resume
def initialize_training_objects(
    cfg: TrainPipelineConfig,
    device: torch.device,
    start_step: int,
    is_distributed: bool = False,
) -> Tuple:
    """
    ç¬¬äºŒé˜¶æ®µï¼šæ ¹æ®é…ç½®åˆå§‹åŒ–æ‰€æœ‰è®­ç»ƒæ‰€éœ€çš„å¯¹è±¡ã€‚
    """
    logging.info("Creating dataset...")
    dataset = make_dataset(cfg)

    logging.info("Creating policy...")
    policy = make_policy(cfg=cfg.policy, ds_meta=dataset.meta)
    policy.to(device)

    # HIGHLIGHT: å°†æ¨¡å‹ç”¨DDPåŒ…è£¹
    if is_distributed:
        # device_ids å°†å½“å‰è¿›ç¨‹çš„GPU rankåŒ…è£¹åœ¨listä¸­
        policy = DDP(policy, device_ids=[device.index], find_unused_parameters=False)
        logging.info(f"Wrapped policy with DDP on device:{device.index}")

    logging.info("Creating optimizer and scheduler...")
    optimizer, lr_scheduler = make_optimizer_and_scheduler(cfg, policy)
    grad_scaler = GradScaler(device.type, enabled=cfg.policy.use_amp)

    # åŠ è½½ Checkpoint (å¦‚æœæ–­ç‚¹ç»­ç»ƒ)
    if start_step > 0:
        checkpoint_path = Path(cfg.output_dir) / "checkpoints" / "last"
        if checkpoint_path.is_symlink() or checkpoint_path.exists():
            logging.info(f"Resuming training from checkpoint: {checkpoint_path.resolve()}")
            # HIGHLIGHT: åŠ è½½æ¨¡å‹çŠ¶æ€æ—¶ï¼Œéœ€è¦åŠ è½½åˆ° DDP åŒ…è£…çš„æ¨¡å‹å†…éƒ¨çš„ `module`
            model_to_load = policy.module if is_distributed else policy
            loaded_step, optimizer, lr_scheduler = load_training_state(
                checkpoint_path, model_to_load, optimizer, lr_scheduler
            )
            
            logging.info(f"Checkpoint loaded, was at step {loaded_step}. Starting from {start_step}.")
        else:
            logging.warning(f"Expected a checkpoint at {checkpoint_path} for resuming, but not found.")

    return dataset, policy, optimizer, lr_scheduler, grad_scaler


async def execute_training_loop(
    cfg: TrainPipelineConfig,
    device: torch.device,
    start_step: int,
    end_step: int,
    training_objects: Tuple,
    is_distributed: bool = False,
    rank: int = 0,
    task_id: int = 0,
):
    """
    ç¬¬ä¸‰é˜¶æ®µï¼šæ‰§è¡Œæ ¸å¿ƒçš„è®­ç»ƒå¾ªç¯ã€‚
    """
    dataset, policy, optimizer, lr_scheduler, grad_scaler = training_objects
    
    sampler = None
    shuffle = True

    # HIGHLIGHT: ä¸º Dataloader è®¾ç½® DistributedSampler
    if is_distributed:
        if hasattr(cfg.policy, "drop_n_last_frames"):
            # æ³¨æ„: EpisodeAwareSampler å’Œ DistributedSampler å¯èƒ½éœ€è¦æ›´å¤æ‚çš„é›†æˆ
            # è¿™é‡Œä¸ºäº†ç®€åŒ–ï¼Œæˆ‘ä»¬ä¼˜å…ˆä½¿ç”¨ DistributedSampler
            logging.warning("EpisodeAwareSampler is not directly compatible with DistributedSampler. Using standard DistributedSampler.")
        
        sampler = DistributedSampler(dataset, shuffle=True)
        shuffle = False # ä½¿ç”¨ sampler æ—¶ï¼Œshuffle å¿…é¡»ä¸º False
        logging.info("Using DistributedSampler for DataLoader.")
    elif hasattr(cfg.policy, "drop_n_last_frames"):
        sampler = EpisodeAwareSampler(
            dataset.episode_data_index, drop_n_last_frames=cfg.policy.drop_n_last_frames, shuffle=True
        )
        shuffle = False
        
    dataloader = torch.utils.data.DataLoader(
        dataset, 
        num_workers=cfg.num_workers,
        batch_size=cfg.batch_size, 
        shuffle=shuffle, 
        sampler=sampler, 
        pin_memory=device.type != "cpu", 
        drop_last=True
    )
    dl_iter = cycle(dataloader)
    
    if start_step > 0:
        logging.info(f"Fast-forwarding dataloader by {start_step} steps...")
        if is_distributed:
            # ä¼°ç®—èµ·å§‹ epochï¼Œä»¥ä¾¿æ­£ç¡®åœ°å¿«è¿› sampler
            # è¿™æ˜¯ä¸€ä¸ªè¿‘ä¼¼å€¼ï¼Œä½†å¯¹äºæ¢å¤éšæœºæ€§å¾ˆé‡è¦
            start_epoch = start_step // len(dataloader)
            sampler.set_epoch(start_epoch)
        for _ in range(start_step):
            next(dl_iter)

    policy.train()

    train_metrics = {
        "loss": AverageMeter("loss", ":.3f"), "grad_norm": AverageMeter("grdn", ":.3f"), "lr": AverageMeter("lr", ":0.1e"),
        "update_s": AverageMeter("updt_s", ":.3f"), "dataloading_s": AverageMeter("data_s", ":.3f"),
    }
    train_tracker = MetricsTracker(
        cfg.batch_size * (dist.get_world_size() if is_distributed else 1),
        dataset.num_frames, dataset.num_episodes, train_metrics, initial_step=start_step
    )

    logging.info(f"Entering training loop from step {start_step} to {end_step}...")
    for step in range(start_step, end_step):
        if is_distributed and sampler:
            cur_epoch = step // len(dataloader)
            sampler.set_epoch(cur_epoch)
        batch = next(dl_iter)

        for key in batch:
            if isinstance(batch[key], torch.Tensor):
                batch[key] = batch[key].to(device, non_blocking=True)

        train_tracker, output_dict = update_policy(
            train_tracker,
            policy,
            batch,
            optimizer,
            cfg.optimizer.grad_clip_norm,
            grad_scaler=grad_scaler,
            lr_scheduler=lr_scheduler,
            use_amp=cfg.policy.use_amp,
        )
        current_step = step + 1

        is_log_step = cfg.log_freq > 0 and current_step % cfg.log_freq == 0
        is_periodic_saving_step = current_step % cfg.save_freq == 0

        # if is_log_step:
        #     log_callback(current_step, train_tracker.to_dict())
        #     train_tracker.reset_averages() 

        # try:
        #     if cfg.save_checkpoint and is_periodic_saving_step:
        #         logging.info(f"Saving checkpoint at step {current_step}")
        #         checkpoint_dir = get_step_checkpoint_dir(cfg.output_dir, cfg.steps, current_step)
        #         logging.info(f"Saving checkpoint to: {checkpoint_dir}")
        #         await async_save_checkpoint(checkpoint_dir, current_step, cfg, policy, optimizer, lr_scheduler)
        #         update_last_checkpoint(checkpoint_dir)
                
        #         save_callback(current_step, str(checkpoint_dir))
            #     except Exception as e:
            # logging.error(f"CRITICAL: `save_checkpoint` failed at step {current_step} with error: {e}", exc_info=True)
            # raise e

        # HIGHLIGHT: åªæœ‰ rank 0 è¿›ç¨‹æ‰§è¡Œæ—¥å¿—è®°å½•å’Œä¿å­˜
        if rank == 0:
            if is_log_step:
                log_dict = train_tracker.to_dict()
                log_msg = f"Step {current_step}: " + ", ".join([f"{k}={v:.4f}" for k, v in log_dict.items()])
                logging.info(log_msg)
                
                # ç›´æ¥è°ƒç”¨ send_log_message å°†æ—¥å¿—å‘é€åˆ° RabbitMQ
                try:
                    loss = log_dict.get('loss', 0.0)
                    await send_log_message(
                        task_id=task_id, epoch=current_step, loss=loss, accuracy=-1.0,
                        log_message=f"Step {current_step}: loss={loss:.4f}, lr={log_dict.get('lr', 0.0):.1e}"
                    )
                except Exception as e:
                    logging.error(f"Failed to send log message to RabbitMQ at step {current_step}: {e}")
                
                train_tracker.reset_averages() 

            try:
                if cfg.save_checkpoint and is_periodic_saving_step:
                    logging.info(f"Saving checkpoint at step {current_step}")
                    checkpoint_dir = get_step_checkpoint_dir(cfg.output_dir, cfg.steps, current_step)
                    logging.info(f"Saving checkpoint to: {checkpoint_dir}")
                    
                    # è·å–å†…éƒ¨æ¨¡å‹è¿›è¡Œä¿å­˜
                    model_to_save = policy.module if is_distributed else policy
                    await async_save_checkpoint(checkpoint_dir, current_step, cfg, model_to_save, optimizer, lr_scheduler)
                    update_last_checkpoint(checkpoint_dir)
                    
            except Exception as e:
                logging.error(f"CRITICAL: `save_checkpoint` failed at step {current_step} with error: {e}", exc_info=True)
                raise e

    # ----- å¾ªç¯ç»“æŸåï¼Œå¼ºåˆ¶ä¿å­˜å½“å‰åˆ†ç‰‡çš„æœ€ç»ˆçŠ¶æ€ -----
    if rank == 0:
        logging.info(f"Finished training slice loop at step {end_step}. Performing final save for this slice.")
        
        last_step_already_saved = (end_step > 0 and end_step % cfg.save_freq == 0)

        try:
            if cfg.save_checkpoint and not last_step_already_saved:
                logging.info(f"Saving final state of the slice at step {end_step}")
                checkpoint_dir = get_step_checkpoint_dir(cfg.output_dir, cfg.steps, end_step)
                logging.info(f"Saving final checkpoint to: {checkpoint_dir}")
                
                model_to_save = policy.module if is_distributed else policy
                await save_checkpoint(checkpoint_dir, end_step, cfg, model_to_save, optimizer, lr_scheduler)
                update_last_checkpoint(checkpoint_dir)
            elif last_step_already_saved:
                logging.info(f"Step {end_step} was already saved as a periodic checkpoint. Skipping duplicate save.")
        except Exception as e:
            logging.error(f"CRITICAL: `save_checkpoint` failed at step {end_step} with error: {e}", exc_info=True)
            raise e

    # HIGHLIGHT: ä½¿ç”¨ barrier ç¡®ä¿æ‰€æœ‰è¿›ç¨‹éƒ½å®Œæˆäº†å®ƒä»¬çš„è®­ç»ƒå¾ªç¯ï¼Œå†ç»“æŸ
    if is_distributed:
        dist.barrier()
    # END HIGHLIGHT

    # # ----- å¾ªç¯ç»“æŸåï¼Œå¼ºåˆ¶ä¿å­˜å½“å‰åˆ†ç‰‡çš„æœ€ç»ˆçŠ¶æ€ -----
    # logging.info(f"Finished training slice loop at step {end_step}. Performing final save for this slice.")
    
    # # æ£€æŸ¥è¿™ä¸€æ­¥æ˜¯å¦å·²ç»åœ¨å¾ªç¯çš„æœ€åä¸€æ¬¡è¿­ä»£ä¸­ä¿å­˜è¿‡äº†
    # last_step_already_saved = (end_step % cfg.save_freq == 0)

    # try:
    #     if cfg.save_checkpoint and not last_step_already_saved:
    #         logging.info(f"Saving final state of the slice at step {end_step}")
    #         checkpoint_dir = get_step_checkpoint_dir(cfg.output_dir, cfg.steps, end_step)
    #         logging.info(f"Saving final checkpoint to: {checkpoint_dir}")
    #         await save_checkpoint(checkpoint_dir, end_step, cfg, policy, optimizer, lr_scheduler)
    #         update_last_checkpoint(checkpoint_dir)
    #         save_callback(end_step, str(checkpoint_dir))
    #     elif last_step_already_saved:
    #         logging.info(f"Step {end_step} was already saved as a periodic checkpoint. Skipping duplicate save.")
    # except Exception as e:
    #     logging.error(f"CRITICAL: `save_checkpoint` failed at step {end_step} with error: {e}", exc_info=True)
    #     raise e

    logging.info(f"Finished training slice. Final step for this slice: {end_step}")
    return end_step


async def run_lerobot_training(
    base_config_path: str,
    user_override_config: Dict[str, Any],
    run_dir: str,
    task_id: int,
    start_step: int,
    end_step: int,
):
    """
    ä¸»å…¥å£å‡½æ•°ï¼Œç¼–æ’æ•´ä¸ªè®­ç»ƒæµç¨‹ã€‚
    """
    # é˜¶æ®µä¸€ï¼šå‡†å¤‡é…ç½®
    rank = int(os.environ.get("RANK", 0))
    local_rank = int(os.environ.get("LOCAL_RANK", 0))
    world_size = int(os.environ.get("WORLD_SIZE", 1))
    
    is_distributed = world_size > 1
    print(f"[Process {rank}/{world_size}] Starting training with is_distributed={is_distributed}, local_rank={local_rank}")

    settings.RABBITMQ_HOST = os.environ.get("RABBITMQ_HOST", "localhost")
    settings.RABBITMQ_USER = os.environ.get("RABBITMQ_USER", "guest")
    settings.RABBITMQ_PASS = os.environ.get("RABBITMQ_PASS", "guest")
    await init_rabbitmq()
    
    if is_distributed:
        dist.init_process_group(backend="nccl", init_method="env://")
        torch.cuda.set_device(local_rank)
        print(f"[Process {rank}/{world_size}] Distributed training initialized. Using GPU: {local_rank}")

    if rank == 0:
        print(f"base_config_path: {base_config_path}")
        print(f"user_override_config: {user_override_config}")
        print(f"run_dir: {run_dir}")
    
    cfg = prepare_config(base_config_path, user_override_config, run_dir, task_id, start_step, end_step, is_distributed)

    print(f"base_config_path: {base_config_path}")
    print(f"user_override_config: {user_override_config}")
    # print(f"cfg: {cfg}")
    print(f"run_dir: {run_dir}")
    # è®¾ç½®è®¾å¤‡
    device = torch.device(f"cuda:{local_rank}" if torch.cuda.is_available() else "cpu")
    torch.backends.cudnn.benchmark = True
    torch.backends.cuda.matmul.allow_tf32 = True

        # æ·»åŠ GPUç›‘æ§ä»£ç 
    if rank == 0:
        print(f"ğŸ” GPUç›‘æ§ä¿¡æ¯:")
        print(f"   - å®é™…ä½¿ç”¨çš„è®¾å¤‡: {device}")
        print(f"   - CUDAå¯ç”¨: {torch.cuda.is_available()}")
        if torch.cuda.is_available():
            print(f"   - GPUæ•°é‡: {torch.cuda.device_count()}")
            print(f"   - å½“å‰GPU: {torch.cuda.current_device()}")
            print(f"   - GPUåç§°: {torch.cuda.get_device_name()}")
            print(f"   - GPUå†…å­˜: {torch.cuda.get_device_properties(0).total_memory / 1024**3:.1f} GB")
    

    # é˜¶æ®µäºŒï¼šåˆå§‹åŒ–æ‰€æœ‰å¯¹è±¡
    training_objects = initialize_training_objects(cfg, device, start_step,is_distributed)
    
    # é˜¶æ®µä¸‰ï¼šæ‰§è¡Œè®­ç»ƒå¾ªç¯
    final_step = await execute_training_loop(
        cfg, device, start_step, end_step,
        training_objects,is_distributed,rank,task_id
    )

    return final_step           


if __name__ == "__main__":
    import asyncio
    # HIGHLIGHT: å¯¼å…¥ argparse ç”¨äºç®€å•çš„å‘½ä»¤è¡Œè§£æ
    import argparse
    import json

    # 1. åˆ›å»ºä¸€ä¸ªè§£æå™¨
    parser = argparse.ArgumentParser(description="LeRobot Distributed Training Logic")

    # 2. æ·»åŠ æ‰€æœ‰æˆ‘ä»¬éœ€è¦çš„å‘½ä»¤è¡Œå‚æ•°
    parser.add_argument("--base_config_path", type=str, required=True)
    parser.add_argument("--run_dir", type=str, required=True)
    parser.add_argument("--task_id", type=int, required=True)
    parser.add_argument("--start_step", type=int, required=True)
    parser.add_argument("--end_step", type=int, required=True)
    parser.add_argument("--user_override_config", type=str, required=True, help="A JSON string of the user override config.")

    # 3. è§£æå‚æ•°
    args = parser.parse_args()

    # 4. å‡†å¤‡å‚æ•°å¹¶è°ƒç”¨ä¸»å‡½æ•°
    try:
        user_override_dict = json.loads(args.user_override_config)
        
        asyncio.run(run_lerobot_training(
            base_config_path=args.base_config_path,
            user_override_config=user_override_dict,
            run_dir=args.run_dir,
            task_id=args.task_id,
            start_step=args.start_step,
            end_step=args.end_step
        ))
    except json.JSONDecodeError:
        logging.error(f"Failed to decode user_override_config JSON string: {args.user_override_config}")
        exit(1)
    except Exception as e:
        logging.error(f"An unexpected error occurred: {e}", exc_info=True)
        exit(1)