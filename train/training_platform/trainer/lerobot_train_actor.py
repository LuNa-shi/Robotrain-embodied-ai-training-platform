# training_platform/trainer/trainer_actor.py

import logging
import ray
import asyncio
import os
import shutil
import functools
from pathlib import Path
import subprocess
import json
import glob

from training_platform.configs.settings import settings
from training_platform.common.task_models import TrainingTask
from training_platform.common.rabbitmq_utils import init_rabbitmq, send_log_message
from training_platform.common.minio_utils import get_minio_client, upload_ckpt_to_minio, download_ckpt_from_minio, download_dataset_from_minio

# å¯¼å…¥æˆ‘ä»¬çš„æ–°è®­ç»ƒé€»è¾‘
# from training_platform.trainer.lerobot_trainer_logic import run_lerobot_training
from lerobot.common.utils.train_utils import TrainPipelineConfig

@ray.remote
class TrainerActor:
    def __init__(self, task: TrainingTask):
        self.task = task
        self.run_dir = os.path.join(settings.RUN_DIR_BASE, str(self.task.task_id))
        os.makedirs(self.run_dir, exist_ok=True)
        
        # è·å–å½“å‰actorçš„äº‹ä»¶å¾ªç¯ï¼Œç”¨äºçº¿ç¨‹å®‰å…¨çš„å›è°ƒ
        self.loop = asyncio.get_running_loop()
        
        # æ·»åŠ ä¸€ä¸ªé›†åˆæ¥è·Ÿè¸ªæ­£åœ¨è¿›è¡Œçš„å¼‚æ­¥ä»»åŠ¡
        # self.pending_uploads = set()

        # Manager æ¨¡å¼ç¡®ä¿åœ¨ Actor å¯åŠ¨æ—¶ï¼Œå¼‚æ­¥åœ°å‡†å¤‡å¥½è¿æ¥
        asyncio.create_task(init_rabbitmq())
        
        print(f"[{self.task.task_id}] Trainer Actor initialized for real training.")

    # async def _log_callback(self, step: int, metrics: dict):
    #     """å¼‚æ­¥æ—¥å¿—å›è°ƒï¼Œå‘é€æ¶ˆæ¯åˆ° RabbitMQã€‚"""
    #     # ä» metrics å­—å…¸ä¸­æå–å…³é”®ä¿¡æ¯
    #     loss = metrics.get('loss', 0.0)
    #     # ä½ å¯ä»¥æ·»åŠ ä»»ä½•ä½ æƒ³ä» lerobot tracker ä¸­è®°å½•çš„æŒ‡æ ‡
    #     log_msg = f"Step {step}: loss={loss:.4f}, lr={metrics.get('lr', 0.0):.1e}"
    #     await send_log_message(
    #         task_id=self.task.task_id,
    #         epoch=step, # åœ¨æˆ‘ä»¬çš„å¹³å°ä¸­ï¼Œstep ç­‰åŒäº epoch
    #         loss=loss,
    #         accuracy=-1.0, # lerobot é€šå¸¸ä¸ç›´æ¥æŠ¥å‘Š accuracyï¼Œè®¾ä¸º-1
    #         log_message=log_msg
    #     )

    # async def _save_checkpoint_callback(self, step: int, checkpoint_dir: str):
    #     """å¼‚æ­¥ä¿å­˜å›è°ƒï¼Œå°† checkpoint ç›®å½•å‹ç¼©å¹¶ä¸Šä¼ åˆ° MinIOã€‚"""
    #     try:
    #         print(f"[{self.task.task_id}] Compressing checkpoint dir: {checkpoint_dir}")
    #         # å°† checkpoint ç›®å½•æ‰“åŒ…æˆ zip æ–‡ä»¶
    #         zip_base_name = os.path.join(self.run_dir, f"checkpoint_step_{step}")
    #         zip_path = shutil.make_archive(zip_base_name, 'zip', checkpoint_dir)
            
    #         # ä¸Šä¼ åˆ° MinIO
    #         minio_client = await get_minio_client()
    #         object_name = f"{self.task.task_id}/checkpoint_step_{step}.zip"
    #         print(f"[{self.task.task_id}] upload checkpoint_object_name: {object_name}")
    #         await upload_ckpt_to_minio(
    #             client=minio_client,
    #             ckpt_file_local_path=zip_path,
    #             filename=object_name,
    #         )
    #         print(f"[{self.task.task_id}] Checkpoint for step {step} uploaded to MinIO.")
    #         # æ¸…ç†æœ¬åœ°çš„ zip æ–‡ä»¶
    #         os.remove(zip_path)

    #     except Exception as e:
    #         print(f"âŒ [{self.task.task_id}] Failed to upload checkpoint for step {step}: {e}")
    #     finally:
    #         # ä»å¾…å¤„ç†é›†åˆä¸­ç§»é™¤ä»»åŠ¡
    #         if step in self.pending_uploads:
    #             self.pending_uploads.remove(step)

    async def _upload_checkpoint_to_minio(self, step: int, checkpoint_dir: str):
        """å°†å•ä¸ªæœ¬åœ° checkpoint ç›®å½•å‹ç¼©å¹¶ä¸Šä¼ åˆ° MinIO"""
        try:
            print(f"[{self.task.task_id}] Compressing and uploading checkpoint dir: {checkpoint_dir}")
            zip_base_name = os.path.join(self.run_dir, f"checkpoint_step_{step}")
            zip_path = shutil.make_archive(zip_base_name, 'zip', checkpoint_dir)
            checkpoint_path = Path(checkpoint_dir)
            minio_client = await get_minio_client()
            object_name = f"{self.task.task_id}/checkpoint_step_{step}.zip"
            print(f"[{self.task.task_id}] Uploading checkpoint object: {object_name}")
            await upload_ckpt_to_minio(client=minio_client, 
                                       ckpt_file_local_path=zip_path, 
                                       filename=object_name)
            print(f"[{self.task.task_id}] Checkpoint for step {step} uploaded to MinIO.")
            
        except Exception as e:
            print(f"âŒ [{self.task.task_id}] Failed to upload checkpoint for step {step}: {e}")
        finally:
            # æ¸…ç†æœ¬åœ°çš„ zip æ–‡ä»¶
            if os.path.exists(zip_path):
                os.remove(zip_path)
            if zip_path and checkpoint_path.exists():
                try:
                    shutil.rmtree(checkpoint_path)
                    print(f"ğŸ§¹ [{self.task.task_id}] Cleaned up local checkpoint directory: {checkpoint_path}")
                except OSError as e:
                    print(f"âš ï¸ [{self.task.task_id}] Error removing checkpoint directory {checkpoint_path}: {e}")
            
    def _determine_base_config_path(self) -> str:
        # è¿™ä¸ªæ–¹æ³•ä¿æŒä¸å˜
        user_conf = self.task.config
        policy_type = user_conf.get("policy", {}).get("type")
        env_type = user_conf.get("env", {}).get("type")

        if not policy_type or not env_type:
            raise ValueError("User config must specify both 'policy.type' and 'env.type'")

        config_filename = f"{policy_type}_{env_type}_config.json"
        
        # ç¡®ä¿è·¯å¾„ç›¸å¯¹äºé¡¹ç›®æ ¹ç›®å½•
        # åœ¨ç”Ÿäº§ä¸­ï¼Œæœ€å¥½ä½¿ç”¨ç»å¯¹è·¯å¾„æˆ–ç¯å¢ƒå˜é‡æ¥å®šä¹‰ config ç›®å½•
        project_root = Path(__file__).resolve().parent.parent.parent
        base_path = project_root / "config"
        config_file_path = base_path / config_filename

        logging.info(f"Determined base config file: {config_file_path}")

        if not config_file_path.exists():
            raise FileNotFoundError(f"Base config file '{config_filename}' not found at '{config_file_path}'")
            
        return str(config_file_path)

    def _load_config_from_checkpoint(self, checkpoint_dir: str) -> dict:
        """ä»checkpointçš„train_config.jsonåŠ è½½é…ç½®"""
        train_config_path = Path(checkpoint_dir) / "pretrained_model" / "train_config.json"
        
        if not train_config_path.exists():
            raise FileNotFoundError(f"Train config not found in checkpoint: {train_config_path}")
        
        print(f"[{self.task.task_id}] Loading config from checkpoint: {train_config_path}")
        
        with open(train_config_path, 'r') as f:
            checkpoint_config = json.load(f)
        
        print(f"[{self.task.task_id}] Loaded config from checkpoint with {len(checkpoint_config)} keys")
        return checkpoint_config

    def _get_checkpoint_config_path(self, checkpoint_dir: str) -> str:
        """ä»checkpointè·å–train_config.jsonæ–‡ä»¶è·¯å¾„ä½œä¸ºåŸºç¡€é…ç½®æ–‡ä»¶"""
        train_config_path = Path(checkpoint_dir) / "pretrained_model" / "train_config.json"
        
        if not train_config_path.exists():
            raise FileNotFoundError(f"Train config not found in checkpoint: {train_config_path}")
        
        return str(train_config_path)

    async def train(self, start_step: int, end_step: int) -> int:
        task_id = self.task.task_id
        print(f"[{task_id}] Starting REAL training slice: step {start_step} -> {end_step}")

        gpu_ids = ray.get_gpu_ids()
        num_gpus = len(gpu_ids)
        if num_gpus == 0:
            raise RuntimeError(f"TrainerActor for task {task_id} was not allocated any GPUs.")
        print(f"[{task_id}] Actor has been allocated {num_gpus} GPU(s): {gpu_ids}")
        try:
            minio_client = await get_minio_client()
            
            # å‡†å¤‡ï¼šä¸‹è½½æ•°æ®é›†ï¼ˆå¦‚æœç¬¬ä¸€æ¬¡è¿è¡Œï¼‰å’Œ checkpoint (å¦‚æœæ–­ç‚¹ç»­ç»ƒ)
            # 1. ä¸‹è½½æ•°æ®é›†
            local_dataset_dir = os.path.join(self.run_dir, "dataset")
            print(f"[{task_id}] Checking local dataset directory: {local_dataset_dir}")
            if not os.path.exists(local_dataset_dir) and start_step == 0:
                print(f"[{task_id}] Downloading dataset...")
                dataset_zip_path = os.path.join(self.run_dir, "dataset.zip")
                
                # ä»åµŒå¥—çš„ config ä¸­è·å– repo_id
                dataset_conf = self.task.config.get("dataset", {})

                dataset_object_name = f"{self.task.dataset_uuid}.zip"
                
                success, _ = await download_dataset_from_minio(
                    client=minio_client,
                    download_local_path=dataset_zip_path,
                    dataset_name=dataset_object_name,
                )
                if not success: raise RuntimeError("Failed to download dataset.")
                
                print(f"[{task_id}] Unpacking dataset...")
                shutil.unpack_archive(dataset_zip_path, local_dataset_dir)
                os.remove(dataset_zip_path)

                # print(f"[{task_id}] --- Verifying dataset structure ---")
                # for root, dirs, files in os.walk(local_dataset_dir):
                #     # åªæ‰“å°ä¸¤å±‚æ·±åº¦
                #     level = root.replace(local_dataset_dir, '').count(os.sep)
                #     if level < 3:
                #         indent = ' ' * 4 * (level)
                #         print(f'{indent}{os.path.basename(root)}/')
                #         sub_indent = ' ' * 4 * (level + 1)
                #         for f in files[:3]: # åªçœ‹å‡ ä¸ªæ–‡ä»¶
                #             print(f'{sub_indent}{f}')
                # print(f"[{task_id}] --- End of structure verification ---")

            # 2. ä¸‹è½½ä¸Šä¸€ä¸ª checkpoint ä»minio ä¸‹è½½åˆ°æœ¬åœ°
            checkpoint_extract_dir = None  # ç”¨äºå­˜å‚¨checkpointè§£å‹ç›®å½•
            if start_step > 0:
                print(f"[{task_id}] Downloading previous checkpoint to resume training...")

                prev_save_step = (start_step // self.task.config.get("save_freq", 1000)) * self.task.config.get("save_freq", 1000)
                if prev_save_step > 0:
                    checkpoint_zip_path = os.path.join(self.run_dir, f"checkpoint_step_{prev_save_step}.zip")
                    checkpoint_object_name = f"{self.task.task_id}/checkpoint_step_{prev_save_step}.zip"
                    print(f"[{task_id}] download checkpoint_object_name: {checkpoint_object_name}")
                    
                    success, _ = await download_ckpt_from_minio(
                        client=minio_client,
                        download_local_path=checkpoint_zip_path,
                        ckpt_name=checkpoint_object_name,
                    )
                    if not success: raise RuntimeError(f"Failed to download checkpoint for step {prev_save_step}.")
                    
                    # HIGHLIGHT: ä¿®å¤10: ä½¿ç”¨ä¸ä¸Šä¼ MinIOç›¸åŒçš„è·¯å¾„ç¡®å®šé€»è¾‘
                    # é¦–å…ˆå°è¯•ä»training_config_info.jsonè¯»å–æ­£ç¡®çš„checkpointè·¯å¾„
                    config_info_path = Path(self.run_dir) / "training_config_info.json"
                    checkpoint_base_dir = None
                    
                    if config_info_path.exists():
                        try:
                            with open(config_info_path, 'r') as f:
                                config_info = json.load(f)
                            
                            output_dir = config_info.get("output_dir")
                            if output_dir:
                                checkpoint_base_dir = Path(output_dir) / "checkpoints"
                                print(f"[{task_id}] Using checkpoint path from config: {checkpoint_base_dir}")
                            else:
                                print(f"[{task_id}] Warning: 'output_dir' not found in config_info.json")
                        except (json.JSONDecodeError, ValueError) as e:
                            print(f"[{task_id}] Warning: Failed to read config_info.json: {e}")
                    
                    # å¦‚æœæ— æ³•ä»é…ç½®æ–‡ä»¶è·å–ï¼Œåˆ™ä½¿ç”¨é»˜è®¤è·¯å¾„
                    if checkpoint_base_dir is None:
                        checkpoint_base_dir = Path(self.run_dir) / "checkpoints"
                        print(f"[{task_id}] Using default checkpoint path: {checkpoint_base_dir}")
                    
                    # ç¡®ä¿checkpointç›®å½•å­˜åœ¨
                    os.makedirs(checkpoint_base_dir, exist_ok=True)
                    
                    # è§£å‹åˆ°æ­£ç¡®çš„checkpointç›®å½•
                    checkpoint_extract_dir = checkpoint_base_dir / f"{prev_save_step:06d}"
                    print(f"[{task_id}] Extracting checkpoint to: {checkpoint_extract_dir}")
                    shutil.unpack_archive(checkpoint_zip_path, checkpoint_extract_dir)
                    os.remove(checkpoint_zip_path)
                    
                    # HIGHLIGHT: ä¿®å¤9: éªŒè¯è§£å‹åçš„ç›®å½•ç»“æ„
                    checkpoint_path = Path(checkpoint_extract_dir)
                    training_state_path = checkpoint_path / "training_state"
                    pretrained_model_path = checkpoint_path / "pretrained_model"
                    
                    # éªŒè¯å¿…éœ€çš„ç›®å½•
                    if not training_state_path.exists():
                        print(f"âŒ [{task_id}] ERROR: Training state directory not found after extraction: {training_state_path}")
                        raise RuntimeError(f"Invalid checkpoint structure: missing training_state directory")
                    
                    if not pretrained_model_path.exists():
                        print(f"âŒ [{task_id}] ERROR: Pretrained model directory not found after extraction: {pretrained_model_path}")
                        raise RuntimeError(f"Invalid checkpoint structure: missing pretrained_model directory")
                    
                    print(f"âœ… [{task_id}] Checkpoint extracted successfully with valid structure")

                    # åˆ›å»º last symlink
                    last_link = checkpoint_base_dir / "last"
                    if last_link.exists(): last_link.unlink()
                    last_link.symlink_to(f"{prev_save_step:06d}")

            # ç¡®å®šä½¿ç”¨å“ªä¸ªé…ç½®å’ŒåŸºç¡€é…ç½®æ–‡ä»¶è·¯å¾„
            if start_step > 0 and checkpoint_extract_dir is not None:
                # ä»checkpointæ¢å¤è®­ç»ƒï¼šä½¿ç”¨checkpointä¸­çš„train_config.jsonä½œä¸ºåŸºç¡€é…ç½®
                print(f"[{task_id}] Resuming training from checkpoint")
                
                # ä»checkpointè·å–train_config.jsonä½œä¸ºåŸºç¡€é…ç½®æ–‡ä»¶
                try:
                    base_config_path = self._get_checkpoint_config_path(str(checkpoint_extract_dir))
                    print(f"[{task_id}] Using checkpoint train_config.json as base config: {base_config_path}")
                except Exception as e:
                    print(f"[{task_id}] Warning: Failed to get train_config.json from checkpoint: {e}")
                    print(f"[{task_id}] Falling back to user config inference")
                    base_config_path = self._determine_base_config_path()
                
                # åˆ›å»ºæ¢å¤è®­ç»ƒçš„é…ç½®ï¼Œæ·»åŠ resume=trueå‚æ•°
                training_config = {
                    "resume": True,  # å…³é”®ï¼šè®¾ç½®resume=true
                    "steps": self.task.config.get("steps", 100000),  # ä¿æŒæ€»æ­¥æ•°
                    "save_freq": self.task.config.get("save_freq", 25000),  # ä¿æŒä¿å­˜é¢‘ç‡
                    "log_freq": self.task.config.get("log_freq", 100),  # ä¿æŒæ—¥å¿—é¢‘ç‡
                    "batch_size": self.task.config.get("batch_size", 8),  # ä¿æŒbatch size
                    # HIGHLIGHT: ä¿®å¤7: ç¡®ä¿æ¢å¤è®­ç»ƒæ—¶ä¼ é€’æ­£ç¡®çš„ç­–ç•¥å’Œç¯å¢ƒé…ç½®
                    "policy": self.task.config.get("policy", {}),
                    "env": self.task.config.get("env", {}),
                }
            else:
                # åˆå§‹è®­ç»ƒï¼šä½¿ç”¨ç”¨æˆ·æä¾›çš„é…ç½®
                print(f"[{task_id}] Starting initial training with user-provided config")
                training_config = self.task.config
                base_config_path = self._determine_base_config_path()
            
            # # å‡†å¤‡çº¿ç¨‹å®‰å…¨çš„å›è°ƒå‡½æ•°
            # def sync_log_callback(step, metrics):
            #     asyncio.run_coroutine_threadsafe(self._log_callback(step, metrics), self.loop)

            # def sync_save_callback(step, ckpt_dir):
            #     # å°†ä»»åŠ¡æ·»åŠ åˆ°å¾…å¤„ç†é›†åˆ
            #     self.pending_uploads.add(step)
            #     # æäº¤å¼‚æ­¥ä»»åŠ¡
            #     asyncio.run_coroutine_threadsafe(self._save_checkpoint_callback(step, ckpt_dir), self.loop)

            # åœ¨ä¸€ä¸ªå•ç‹¬çš„çº¿ç¨‹ä¸­è¿è¡ŒåŒæ­¥çš„è®­ç»ƒä»£ç ï¼Œé˜²æ­¢é˜»å¡ Actor
            # final_step = await asyncio.to_thread(
                # run_lerobot_training,
                # base_config_path,
                # training_config,  # ä½¿ç”¨ç¡®å®šçš„é…ç½®
                # self.run_dir,
                # self.task.task_id,
                # start_step,
                # end_step,
                # sync_log_callback,
                # sync_save_callback,
            # )
            # final_step = await run_lerobot_training(
            #     base_config_path=base_config_path,
            #     user_override_config=training_config,  # ä½¿ç”¨ç¡®å®šçš„é…ç½®
            #     run_dir=self.run_dir,
            #     task_id=self.task.task_id,
            #     start_step=start_step,
            #     end_step=end_step,
            #     log_callback=sync_log_callback,
            #     save_callback=sync_save_callback,
            # )

            logic_script_path = Path(__file__).parent / "lerobot_trainer_logic.py"

            # å‡†å¤‡ä¼ é€’ç»™è„šæœ¬çš„å‚æ•°
            # å°† user_override_config å­—å…¸è½¬ä¸º JSON å­—ç¬¦ä¸²
            user_override_config_str = json.dumps(training_config)

            cmd = [
                "torchrun",
                f"--nproc_per_node={num_gpus}",
                str(logic_script_path),
                f"--base_config_path={base_config_path}",
                f"--run_dir={self.run_dir}",
                f"--task_id={task_id}",
                f"--start_step={start_step}",
                f"--end_step={end_step}",
                f"--user_override_config={user_override_config_str}"
            ]
            
            print(f"[{task_id}] Executing command: {' '.join(cmd)}")

            # æ‰§è¡Œå‘½ä»¤å¹¶å®æ—¶æ‰“å°è¾“å‡º
            process = await asyncio.create_subprocess_exec(
                *cmd,
                stdout=asyncio.subprocess.PIPE,
                stderr=asyncio.subprocess.PIPE
            )

            async def log_stream(stream, prefix):
                async for line in stream: print(f"[{task_id}-{prefix}] {line.decode().strip()}")
            
            await asyncio.gather(log_stream(process.stdout, "stdout"), log_stream(process.stderr, "stderr"))
            await process.wait()
            
            if process.returncode != 0:
                raise RuntimeError(f"Training subprocess failed with exit code {process.returncode}")

            # HIGHLIGHT: 4. Read config from the sidecar file and upload checkpoints
            print(f"[{task_id}] Training subprocess finished. Reading config info...")
            
            checkpoint_base_dir = None
            config_info_path = Path(self.run_dir) / "training_config_info.json"

            if not config_info_path.exists():
                print(f"âš ï¸ [{task_id}] Warning: Could not find '{config_info_path}'.")
                print(f"[{task_id}] Falling back to run_dir for checkpoints. This may not find them.")
                checkpoint_base_dir = Path(self.run_dir) / "checkpoints"
            else:
                try:
                    with open(config_info_path, 'r') as f:
                        config_info = json.load(f)
                    
                    # This is the correct, dynamically determined output directory
                    output_dir = config_info.get("output_dir")
                    if not output_dir:
                        raise ValueError("'output_dir' not found in config_info.json")
                        
                    checkpoint_base_dir = Path(output_dir) / "checkpoints"
                    print(f"[{task_id}] Successfully read config. Checkpoint base dir: {checkpoint_base_dir}")

                except (json.JSONDecodeError, ValueError) as e:
                    print(f"âŒ [{task_id}] Failed to read or parse '{config_info_path}': {e}")
                    # In a production system, you might want to raise an exception here.
                    # For now, we'll print an error and stop the upload process for this slice.
                    return end_step # Or raise an exception

            print(f"[{task_id}] Scanning for new checkpoints to upload in '{checkpoint_base_dir}'...")
            
            if checkpoint_base_dir and checkpoint_base_dir.exists():
                # The rest of the logic can remain the same, as it now uses the correct base path.
                all_ckpts = sorted(glob.glob(str(checkpoint_base_dir / "[0-9]*")), key=os.path.getmtime)
                
                # Filter for checkpoints created within the current training slice
                new_ckpts_to_upload = [
                    d for d in all_ckpts if start_step < int(Path(d).name) <= end_step
                ]
                
                # Ensure the final step's checkpoint is included if it was saved
                final_ckpt_dir = checkpoint_base_dir / f"{end_step:06d}"
                if final_ckpt_dir.exists() and str(final_ckpt_dir) not in new_ckpts_to_upload:
                    new_ckpts_to_upload.append(str(final_ckpt_dir))

                if not new_ckpts_to_upload:
                    print(f"[{task_id}] No new checkpoints found in the range ({start_step}, {end_step}].")
                else:
                    print(f"[{task_id}] Found {len(set(new_ckpts_to_upload))} new checkpoint(s) to upload.")
                    for ckpt_dir in set(new_ckpts_to_upload):
                        step = int(Path(ckpt_dir).name)
                        await self._upload_checkpoint_to_minio(step, ckpt_dir)
            else:
                print(f"[{task_id}] Checkpoint directory '{checkpoint_base_dir}' does not exist. No checkpoints to upload.")


            return end_step

        except Exception as e:
            print(f"âŒ [{task_id}] Training failed with an exception: {e}")
            # å¯ä»¥åœ¨è¿™é‡Œåšæ›´è¯¦ç»†çš„é”™è¯¯å¤„ç†
            raise e