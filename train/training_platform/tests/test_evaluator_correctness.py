#!/usr/bin/env python3
"""
ä¿®æ­£åçš„ evaluator_logic æ¨¡å—æµ‹è¯•è„šæœ¬
ä½¿ç”¨ lerobot/act_aloha_sim_insertion_human æ¨¡å‹è¿›è¡Œæµ‹è¯•
"""

import sys
import os
import json
import tempfile
import shutil
import time
import logging
from pathlib import Path
from typing import Dict, Any, Optional

# æ·»åŠ é¡¹ç›®è·¯å¾„
sys.path.insert(0, str(Path(__file__).parent))

# é…ç½®æ—¥å¿—
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)

def test_imports():
    """æµ‹è¯•å¿…è¦çš„æ¨¡å—å¯¼å…¥"""
    print("\nğŸ§ª æµ‹è¯•æ¨¡å—å¯¼å…¥...")
    
    try:
        # æµ‹è¯•æ ¸å¿ƒæ¨¡å—å¯¼å…¥
        from training_platform.evaluator.evaluator_logic import (
            run_lerobot_evaluation,
            prepare_eval_config,
            eval_policy,
            rollout
        )
        
        # æµ‹è¯• LeRobot æ¨¡å—å¯¼å…¥
        from lerobot.common.policies.factory import make_policy
        from lerobot.common.envs.factory import make_env, make_env_config
        from lerobot.common.policies.act.modeling_act import ACTPolicy
        from lerobot.configs.eval import EvalPipelineConfig
        
        print("âœ… æ‰€æœ‰å¿…è¦æ¨¡å—å¯¼å…¥æˆåŠŸ")
        return True
        
    except Exception as e:
        print(f"âŒ æ¨¡å—å¯¼å…¥å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
        return False

def test_model_loading():
    """æµ‹è¯•æŒ‡å®šé¢„è®­ç»ƒæ¨¡å‹çš„åŠ è½½"""
    print("\nğŸ§ª æµ‹è¯•æ¨¡å‹åŠ è½½...")
    
    try:
        from lerobot.common.policies.act.modeling_act import ACTPolicy
        
        model_id = "lerobot/act_aloha_sim_insertion_human"
        print(f"å°è¯•åŠ è½½æ¨¡å‹: {model_id}")
        
        # æµ‹è¯•æ¨¡å‹æ˜¯å¦å¯ä»¥æˆåŠŸåŠ è½½
        policy = ACTPolicy.from_pretrained(model_id)
        
        print(f"âœ… æ¨¡å‹åŠ è½½æˆåŠŸ: {type(policy)}")
        print(f"æ¨¡å‹è®¾å¤‡: {next(policy.parameters()).device}")
        print(f"æ¨¡å‹å‚æ•°æ•°é‡: {sum(p.numel() for p in policy.parameters() if p.requires_grad):,}")
        
        return True
        
    except Exception as e:
        print(f"âŒ æ¨¡å‹åŠ è½½å¤±è´¥: {e}")
        print("è¿™å¯èƒ½æ˜¯å› ä¸ºç½‘ç»œé—®é¢˜æˆ–æ¨¡å‹ä¸å¯ç”¨")
        return False

def test_environment_creation():
    """æµ‹è¯•ç¯å¢ƒåˆ›å»º"""
    print("\nğŸ§ª æµ‹è¯•ç¯å¢ƒåˆ›å»º...")
    
    try:
        from lerobot.common.envs.factory import make_env, make_env_config
        
        # ä½¿ç”¨æ­£ç¡®çš„ç¯å¢ƒé…ç½®æ–¹å¼
        env_cfg = make_env_config(
            env_type="aloha",
            task="AlohaInsertion-v0",
            fps=50,
            episode_length=400,
            obs_type="pixels_agent_pos",
            render_mode="rgb_array"
        )
        
        # åˆ›å»ºç¯å¢ƒ
        env = make_env(env_cfg, n_envs=2, use_async_envs=False)
        
        print(f"âœ… ç¯å¢ƒåˆ›å»ºæˆåŠŸ: {type(env)}")
        print(f"ç¯å¢ƒæ•°é‡: {env.num_envs}")
        
        # æµ‹è¯•ç¯å¢ƒé‡ç½®
        obs, info = env.reset()
        print(f"è§‚å¯Ÿå½¢çŠ¶ç±»å‹: {type(obs)}")
        if isinstance(obs, dict):
            for key, value in obs.items():
                if hasattr(value, 'shape'):
                    print(f"  {key}: {value.shape}")
                else:
                    print(f"  {key}: {type(value)}")
        
        env.close()
        return True
        
    except Exception as e:
        print(f"âŒ ç¯å¢ƒåˆ›å»ºå¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
        return False

def run_evaluator_logic_test():
    """è¿è¡Œå®Œæ•´çš„è¯„ä¼°é€»è¾‘æµ‹è¯•"""
    print("\nğŸ§ª è¿è¡Œå®Œæ•´è¯„ä¼°é€»è¾‘æµ‹è¯•...")
    
    try:
        from training_platform.evaluator.evaluator_logic import run_lerobot_evaluation
        
        # é…ç½®
        model_path = "lerobot/act_aloha_sim_insertion_human"
        
        # ä¿®æ­£åçš„ç¯å¢ƒé…ç½® - å¿…é¡»åŒ…å« type å­—æ®µ
        env_config = {
            "type": "aloha",
            "task": "AlohaInsertion-v0",
            "fps": 50,
            "episode_length": 400,
            "obs_type": "pixels_agent_pos",
            "render_mode": "rgb_array"
        }
        
        eval_config = {
            "n_episodes": 4,  # è¾ƒå°‘çš„å‰§é›†æ•°ç”¨äºå¿«é€Ÿæµ‹è¯•
            "batch_size": 2,
            "use_async_envs": False
        }
        
        # åˆ›å»ºä¸´æ—¶è¾“å‡ºç›®å½•
        with tempfile.TemporaryDirectory() as temp_dir:
            output_dir = str(Path(temp_dir) / "eval_output")
            
            print(f"å¼€å§‹è¯„ä¼°ï¼Œè¾“å‡ºç›®å½•: {output_dir}")
            start_time = time.time()
            
            # è¿è¡Œè¯„ä¼°
            results = run_lerobot_evaluation(
                model_path=model_path,
                env_config=env_config,
                eval_config=eval_config,
                output_dir=output_dir,
                seed=1000,
                max_episodes_rendered=2,  # æ¸²æŸ“å°‘é‡è§†é¢‘è¿›è¡Œæµ‹è¯•
                return_episode_data=False
            )
            
            end_time = time.time()
            eval_duration = end_time - start_time
            
            # éªŒè¯ç»“æœ
            assert "aggregated" in results, "ç»“æœä¸­ç¼ºå°‘ 'aggregated' é”®"
            assert "per_episode" in results, "ç»“æœä¸­ç¼ºå°‘ 'per_episode' é”®"
            
            aggregated = results["aggregated"]
            required_keys = ["avg_sum_reward", "avg_max_reward", "pc_success", "eval_s", "eval_ep_s"]
            
            for key in required_keys:
                assert key in aggregated, f"èšåˆç»“æœä¸­ç¼ºå°‘ '{key}' é”®"
            
            print("âœ… è¯„ä¼°å®Œæˆï¼")
            print(f"æ€»è€—æ—¶: {eval_duration:.2f} ç§’")
            print("\nğŸ“Š è¯„ä¼°ç»“æœ:")
            print(f"  å¹³å‡æ€»å¥–åŠ±: {aggregated['avg_sum_reward']:.4f}")
            print(f"  å¹³å‡æœ€å¤§å¥–åŠ±: {aggregated['avg_max_reward']:.4f}")
            print(f"  æˆåŠŸç‡: {aggregated['pc_success']:.2f}%")
            print(f"  è¯„ä¼°æ—¶é—´: {aggregated['eval_s']:.2f} ç§’")
            print(f"  æ¯å‰§é›†å¹³å‡æ—¶é—´: {aggregated['eval_ep_s']:.2f} ç§’")
            
            print(f"\nğŸ“‹ å„å‰§é›†è¯¦ç»†ç»“æœ:")
            for i, episode in enumerate(results["per_episode"]):
                print(f"  å‰§é›† {i}: æ€»å¥–åŠ±={episode['sum_reward']:.4f}, "
                      f"æœ€å¤§å¥–åŠ±={episode['max_reward']:.4f}, "
                      f"æˆåŠŸ={episode['success']}")
            
            # æ£€æŸ¥è¾“å‡ºæ–‡ä»¶
            eval_info_file = Path(output_dir) / "eval_info.json"
            if eval_info_file.exists():
                print(f"\nğŸ“ è¯„ä¼°ä¿¡æ¯å·²ä¿å­˜: {eval_info_file}")
                with open(eval_info_file) as f:
                    saved_results = json.load(f)
                    print("âœ… ä¿å­˜çš„ç»“æœæ–‡ä»¶æ ¼å¼æ­£ç¡®")
            
            # æ£€æŸ¥è§†é¢‘æ–‡ä»¶
            videos_dir = Path(output_dir) / "videos"
            if videos_dir.exists():
                video_files = list(videos_dir.glob("*.mp4"))
                print(f"ğŸ“¹ ç”Ÿæˆçš„è§†é¢‘æ–‡ä»¶: {len(video_files)} ä¸ª")
                for video_file in video_files:
                    print(f"  - {video_file.name}")
            
            return True
            
    except Exception as e:
        print(f"âŒ è¯„ä¼°é€»è¾‘æµ‹è¯•å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
        return False

def run_simple_test():
    """åŸºäºæä¾›çš„æµ‹è¯•æ–‡ä»¶çš„ç®€åŒ–æµ‹è¯•"""
    print("\nğŸ§ª è¿è¡Œç®€åŒ–æµ‹è¯•ï¼ˆåŸºäºæä¾›çš„æµ‹è¯•æ–‡ä»¶ï¼‰...")
    
    try:
        from training_platform.evaluator.evaluator_logic import run_lerobot_evaluation
        
        # åŸºäºé™„ä»¶æµ‹è¯•æ–‡ä»¶ï¼Œä½†ä¿®æ­£é…ç½®
        model_path = "lerobot/act_aloha_sim_insertion_human"
        output_dir = Path("./outputs/eval/aloha_sim_insertion_human")
        
        # æ¸…ç†ä¹‹å‰çš„è¾“å‡º
        if output_dir.exists():
            shutil.rmtree(output_dir)
        
        # ä¿®æ­£åçš„é…ç½®
        env_config = {
            "type": "aloha",  # æ·»åŠ å¿…éœ€çš„ type å­—æ®µ
            "task": "AlohaInsertion-v0",  # ä½¿ç”¨æ­£ç¡®çš„ä»»åŠ¡å
            "fps": 50,
            "episode_length": 400,
            "obs_type": "pixels_agent_pos", 
            "render_mode": "rgb_array"
        }
        
        eval_config = {
            "n_episodes": 4,
            "batch_size": 2,
            "use_async_envs": False
        }
        
        print(f"è¿è¡Œè¯„ä¼°ï¼šæ¨¡å‹ {model_path}")
        print(f"ç¯å¢ƒï¼š{env_config['task']}")
        print(f"å‰§é›†æ•°ï¼š{eval_config['n_episodes']}")
        
        # è¿è¡Œè¯„ä¼°
        eval_results = run_lerobot_evaluation(
            model_path=model_path,
            env_config=env_config,
            eval_config=eval_config,
            output_dir=str(output_dir),
            max_episodes_rendered=2,
            seed=42
        )
        
        print("\n--- è¯„ä¼°å®Œæˆ ---")
        print("\nèšåˆç»“æœ:")
        for key, value in eval_results.get("aggregated", {}).items():
            print(f"  {key}: {value}")
        
        print("\nå„å‰§é›†ç»“æœ:")
        for i, episode in enumerate(eval_results.get("per_episode", [])[:2]):
            print(f"  å‰§é›† {i}: {episode}")
        
        # éªŒè¯æ–‡ä»¶
        eval_info_file = output_dir / "eval_info.json"
        assert eval_info_file.exists(), "è¯„ä¼°ä¿¡æ¯æ–‡ä»¶æœªåˆ›å»º"
        print(f"âœ… è¯„ä¼°ä¿¡æ¯æ–‡ä»¶å·²ä¿å­˜ï¼š{eval_info_file}")
        
        video_dir = output_dir / "videos"
        if video_dir.exists():
            videos = list(video_dir.glob("*.mp4"))
            print(f"âœ… ç”Ÿæˆäº† {len(videos)} ä¸ªè§†é¢‘æ–‡ä»¶")
        
        return True
        
    except Exception as e:
        print(f"âŒ ç®€åŒ–æµ‹è¯•å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
        return False

def run_comprehensive_test():
    """è¿è¡Œç»¼åˆæµ‹è¯•"""
    print("ğŸš€ å¼€å§‹ evaluator_logic æ¨¡å—æ­£ç¡®æ€§æµ‹è¯•")
    print("=" * 60)
    
    tests = [
        ("æ¨¡å—å¯¼å…¥", test_imports),
        ("é¢„è®­ç»ƒæ¨¡å‹åŠ è½½", test_model_loading),
        ("ç¯å¢ƒåˆ›å»º", test_environment_creation),
        ("å®Œæ•´è¯„ä¼°æµç¨‹", run_evaluator_logic_test),
        ("ç®€åŒ–æµ‹è¯•", run_simple_test),
    ]
    
    passed = 0
    total = len(tests)
    
    for test_name, test_func in tests:
        print(f"\n{'='*20} {test_name} {'='*20}")
        
        try:
            if test_func():
                passed += 1
                print(f"âœ… {test_name} æµ‹è¯•é€šè¿‡")
            else:
                print(f"âŒ {test_name} æµ‹è¯•å¤±è´¥")
                # å¦‚æœæ ¸å¿ƒæµ‹è¯•å¤±è´¥ï¼Œå¯èƒ½éœ€è¦è·³è¿‡åç»­æµ‹è¯•
                if test_name in ["æ¨¡å—å¯¼å…¥", "é¢„è®­ç»ƒæ¨¡å‹åŠ è½½"]:
                    print("âš ï¸  æ ¸å¿ƒåŠŸèƒ½æµ‹è¯•å¤±è´¥ï¼Œè·³è¿‡åç»­æµ‹è¯•")
                    break
        except Exception as e:
            print(f"âŒ {test_name} æµ‹è¯•å¼‚å¸¸: {e}")
            if test_name in ["æ¨¡å—å¯¼å…¥", "é¢„è®­ç»ƒæ¨¡å‹åŠ è½½"]:
                print("âš ï¸  æ ¸å¿ƒåŠŸèƒ½æµ‹è¯•å¤±è´¥ï¼Œè·³è¿‡åç»­æµ‹è¯•")
                break
    
    print(f"\n{'='*60}")
    print(f"æµ‹è¯•ç»“æœ: {passed}/{total} é€šè¿‡")
    print(f"{'='*60}")
    
    if passed == total:
        print("ğŸ‰ æ‰€æœ‰æµ‹è¯•é€šè¿‡ï¼evaluator_logic æ¨¡å—å·¥ä½œæ­£å¸¸ã€‚")
        print("\nğŸ“‹ æµ‹è¯•æ€»ç»“:")
        print("âœ… æ¨¡å—å¯¼å…¥æ­£å¸¸")
        print("âœ… é¢„è®­ç»ƒæ¨¡å‹åŠ è½½æˆåŠŸ")
        print("âœ… ç¯å¢ƒåˆ›å»ºå’Œé…ç½®æ­£ç¡®") 
        print("âœ… å®Œæ•´è¯„ä¼°æµç¨‹è¿è¡ŒæˆåŠŸ")
        print("âœ… è¯„ä¼°ç»“æœæ ¼å¼æ­£ç¡®")
        print("âœ… è¾“å‡ºæ–‡ä»¶ä¿å­˜æ­£å¸¸")
        
        print("\nğŸ¯ æ¨¡å‹è¯„ä¼°ç»“æœè¡¨æ˜:")
        print("â€¢ evaluator_logic æ¨¡å—çš„æ ¸å¿ƒåŠŸèƒ½æ­£å¸¸")
        print("â€¢ èƒ½å¤ŸæˆåŠŸåŠ è½½å’Œä½¿ç”¨é¢„è®­ç»ƒæ¨¡å‹")
        print("â€¢ è¯„ä¼°æµç¨‹å®Œæ•´ï¼Œç»“æœå¯ä¿¡")
        print("â€¢ å¯ä»¥ç”¨äºç”Ÿäº§ç¯å¢ƒçš„æ¨¡å‹è¯„ä¼°")
        
    else:
        print("âš ï¸  éƒ¨åˆ†æµ‹è¯•å¤±è´¥ï¼Œè¯·æ£€æŸ¥ç›¸å…³åŠŸèƒ½ã€‚")
        print("\nğŸ”§ å¯èƒ½çš„è§£å†³æ–¹æ¡ˆ:")
        print("1. æ£€æŸ¥ç½‘ç»œè¿æ¥ï¼Œç¡®ä¿èƒ½è®¿é—® Hugging Face Hub")
        print("2. ç¡®ä¿å®‰è£…äº†æ‰€æœ‰å¿…è¦çš„ä¾èµ–åŒ…")
        print("3. æ£€æŸ¥ LeRobot ç¯å¢ƒé…ç½®æ˜¯å¦æ­£ç¡®")
        print("4. éªŒè¯æ¨¡å‹ ID æ˜¯å¦å­˜åœ¨å’Œå¯è®¿é—®")
    
    return passed == total

if __name__ == "__main__":
    success = run_comprehensive_test()
    
    if success:
        print("\nğŸŠ evaluator_logic æ¨¡å—éªŒè¯å®Œæˆï¼ŒåŠŸèƒ½æ­£å¸¸ï¼")
    else:
        print("\nâŒ evaluator_logic æ¨¡å—éªŒè¯å¤±è´¥ï¼Œéœ€è¦è¿›ä¸€æ­¥è°ƒè¯•ã€‚")
    
    sys.exit(0 if success else 1)