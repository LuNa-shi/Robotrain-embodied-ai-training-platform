#!/usr/bin/env python3
"""
æµ‹è¯• EvaluatorActor çš„å®Œæ•´æµç¨‹
åŒ…æ‹¬ MinIO æ¨¡å‹ä¸‹è½½å’Œè¯„ä¼°æ‰§è¡Œ
"""

import sys
import os
import asyncio
import ray
import tempfile
import shutil
import json
from pathlib import Path
from typing import Dict, Any

# æ·»åŠ é¡¹ç›®è·¯å¾„
sys.path.insert(0, str(Path(__file__).parent))

# å¯¼å…¥å¿…è¦çš„æ¨¡å—
from training_platform.common.task_models import EvaluationTask
from training_platform.evaluator.lerobot_evaluate_actor import EvaluatorActor
from training_platform.configs.settings import settings

def setup_test_environment():
    """è®¾ç½®æµ‹è¯•ç¯å¢ƒ"""
    print("ğŸ”§ è®¾ç½®æµ‹è¯•ç¯å¢ƒ...")
    
    # åˆå§‹åŒ– Rayï¼ˆå¦‚æœå°šæœªåˆå§‹åŒ–ï¼‰
    if not ray.is_initialized():
        ray.init(ignore_reinit_error=True)
        print("âœ… Ray åˆå§‹åŒ–å®Œæˆ")
    
    # åˆ›å»ºæµ‹è¯•ç›®å½•
    test_dir = Path("./test_evaluator_output")
    if test_dir.exists():
        shutil.rmtree(test_dir)
    test_dir.mkdir(exist_ok=True)
    print(f"âœ… æµ‹è¯•ç›®å½•åˆ›å»º: {test_dir}")
    
    # åˆ›å»º outputs æ–‡ä»¶å¤¹ç”¨äºä¿å­˜è¯„ä¼°ç»“æœè§†é¢‘
    outputs_dir = Path("./outputs")
    outputs_dir.mkdir(exist_ok=True)
    print(f"âœ… è¾“å‡ºç›®å½•åˆ›å»º: {outputs_dir}")
    
    # åˆ›å»ºè§†é¢‘å­ç›®å½•
    video_outputs_dir = outputs_dir / "evaluation_videos"
    video_outputs_dir.mkdir(exist_ok=True)
    print(f"âœ… è§†é¢‘è¾“å‡ºç›®å½•åˆ›å»º: {video_outputs_dir}")
    
    return test_dir, outputs_dir, video_outputs_dir

def cleanup_test_environment(test_dir: Path):
    """æ¸…ç†æµ‹è¯•ç¯å¢ƒ"""
    print("ğŸ§¹ æ¸…ç†æµ‹è¯•ç¯å¢ƒ...")
    
    try:
        if test_dir.exists():
            shutil.rmtree(test_dir)
            print(f"âœ… æµ‹è¯•ç›®å½•å·²æ¸…ç†: {test_dir}")
    except Exception as e:
        print(f"âš ï¸  æ¸…ç†å¤±è´¥: {e}")

def copy_videos_to_outputs(source_dir: Path, outputs_dir: Path, task_name: str) -> list:
    """å¤åˆ¶è§†é¢‘æ–‡ä»¶åˆ°è¾“å‡ºç›®å½•"""
    copied_videos = []
    
    try:
        videos_dir = source_dir / "videos"
        if videos_dir.exists():
            video_files = list(videos_dir.glob("*.mp4"))
            print(f"ğŸ“¹ å‘ç° {len(video_files)} ä¸ªè§†é¢‘æ–‡ä»¶")
            
            for i, video_file in enumerate(video_files):
                # åˆ›å»ºå¸¦æ—¶é—´æˆ³çš„æ–‡ä»¶å
                import time
                timestamp = int(time.time())
                output_filename = f"{task_name}_episode_{i}_{timestamp}.mp4"
                output_path = outputs_dir / output_filename
                
                # å¤åˆ¶è§†é¢‘æ–‡ä»¶
                shutil.copy2(video_file, output_path)
                copied_videos.append(output_path)
                print(f"âœ… è§†é¢‘å·²ä¿å­˜: {output_path}")
        
        else:
            print("âš ï¸  æœªæ‰¾åˆ°è§†é¢‘ç›®å½•")
            
    except Exception as e:
        print(f"âŒ å¤åˆ¶è§†é¢‘æ–‡ä»¶å¤±è´¥: {e}")
    
    return copied_videos

async def test_hf_model_evaluation(video_outputs_dir: Path = None):
    """æµ‹è¯•ä½¿ç”¨ Hugging Face æ¨¡å‹çš„è¯„ä¼°æµç¨‹ï¼ˆæ— éœ€ MinIOï¼‰"""
    print("\nğŸ§ª æµ‹è¯• Hugging Face æ¨¡å‹è¯„ä¼°æµç¨‹...")
    
    # åˆ›å»ºè¯„ä¼°ä»»åŠ¡ï¼ˆä½¿ç”¨ HF æ¨¡å‹ IDï¼‰
    eval_task = EvaluationTask(
        task_id=1001,
        user_id=1,
        model_uuid="lerobot/act_aloha_sim_insertion_human",  # HF æ¨¡å‹ ID
        model_type="act",
        env_config={
            "type": "aloha",
            "task": "AlohaInsertion-v0", 
            "fps": 50,
            "episode_length": 400,
            "obs_type": "pixels_agent_pos",
            "render_mode": "rgb_array"
        },
        eval_config={
            "n_episodes": 3,  # å¢åŠ å‰§é›†æ•°ä»¥ç”Ÿæˆæ›´å¤šè§†é¢‘
            "batch_size": 1,
            "use_async_envs": False
        },
        seed=1000,
        max_episodes_rendered=3,  # å¢åŠ æ¸²æŸ“çš„å‰§é›†æ•°
        return_episode_data=False
    )
    
    print(f"ğŸ“‹ è¯„ä¼°ä»»åŠ¡é…ç½®:")
    print(f"  ä»»åŠ¡ ID: {eval_task.task_id}")
    print(f"  æ¨¡å‹: {eval_task.model_uuid}")
    print(f"  ç¯å¢ƒ: {eval_task.env_config['task']}")
    print(f"  å‰§é›†æ•°: {eval_task.eval_config['n_episodes']}")
    
    try:
        # åˆ›å»ºè¯„ä¼° Actor
        evaluator = EvaluatorActor.remote(eval_task)
        print("âœ… EvaluatorActor åˆ›å»ºæˆåŠŸ")
        
        # æ‰§è¡Œè¯„ä¼°ï¼ˆä½¿ç”¨ ray.get è€Œä¸æ˜¯ awaitï¼‰
        print("ğŸš€ å¼€å§‹æ‰§è¡Œè¯„ä¼°...")
        results = ray.get(evaluator.evaluate.remote())
        
        print("âœ… è¯„ä¼°å®Œæˆï¼")
        
        # æ‰“å°è¯„ä¼°ç»“æœ
        if results and "aggregated" in results:
            aggregated = results["aggregated"]
            print(f"\nğŸ“Š è¯„ä¼°ç»“æœ:")
            print(f"  å¹³å‡æ€»å¥–åŠ±: {aggregated.get('avg_sum_reward', 0):.4f}")
            print(f"  å¹³å‡æœ€å¤§å¥–åŠ±: {aggregated.get('avg_max_reward', 0):.4f}")
            print(f"  æˆåŠŸç‡: {aggregated.get('pc_success', 0):.2f}%")
            print(f"  è¯„ä¼°æ—¶é—´: {aggregated.get('eval_s', 0):.2f} ç§’")
            print(f"  æ¯å‰§é›†æ—¶é—´: {aggregated.get('eval_ep_s', 0):.2f} ç§’")
            
            # æ‰“å°å„å‰§é›†ç»“æœ
            print(f"\nğŸ“‹ å„å‰§é›†è¯¦ç»†ç»“æœ:")
            for i, episode in enumerate(results.get("per_episode", [])):
                print(f"  å‰§é›† {i}: æ€»å¥–åŠ±={episode.get('sum_reward', 0):.4f}, "
                      f"æœ€å¤§å¥–åŠ±={episode.get('max_reward', 0):.4f}, "
                      f"æˆåŠŸ={episode.get('success', False)}")
        
        # æ¸…ç† Actor
        ray.get(evaluator.cleanup.remote())
        print("âœ… Actor æ¸…ç†å®Œæˆ")
        
        return True
        
    except Exception as e:
        print(f"âŒ è¯„ä¼°æµ‹è¯•å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
        return False

async def test_direct_evaluation():
    """æµ‹è¯•ç›´æ¥è°ƒç”¨è¯„ä¼°é€»è¾‘ï¼ˆä¸ä½¿ç”¨ Actorï¼‰"""
    print("\nğŸ§ª æµ‹è¯•ç›´æ¥è¯„ä¼°è°ƒç”¨...")
    
    try:
        from training_platform.evaluator.evaluator_logic import run_lerobot_evaluation
        
        # é…ç½®
        model_path = "lerobot/act_aloha_sim_insertion_human"
        env_config = {
            "type": "aloha",
            "task": "AlohaInsertion-v0",
            "fps": 50,
            "episode_length": 400,
            "obs_type": "pixels_agent_pos",
            "render_mode": "rgb_array"
        }
        eval_config = {
            "n_episodes": 2,
            "batch_size": 1,
            "use_async_envs": False
        }
        
        # åˆ›å»ºä¸´æ—¶è¾“å‡ºç›®å½•
        with tempfile.TemporaryDirectory() as temp_dir:
            output_dir = str(Path(temp_dir) / "eval_output")
            
            print(f"ğŸš€ å¼€å§‹ç›´æ¥è¯„ä¼°...")
            print(f"  æ¨¡å‹: {model_path}")
            print(f"  è¾“å‡ºç›®å½•: {output_dir}")
            
            # åœ¨çº¿ç¨‹ä¸­è¿è¡Œè¯„ä¼°
            results = await asyncio.to_thread(
                run_lerobot_evaluation,
                model_path=model_path,
                env_config=env_config,
                eval_config=eval_config,
                output_dir=output_dir,
                seed=1000,
                max_episodes_rendered=1,
                return_episode_data=False
            )
            
            print("âœ… ç›´æ¥è¯„ä¼°å®Œæˆï¼")
            
            # éªŒè¯ç»“æœ
            assert "aggregated" in results
            assert "per_episode" in results
            
            aggregated = results["aggregated"]
            print(f"\nğŸ“Š ç›´æ¥è¯„ä¼°ç»“æœ:")
            print(f"  å¹³å‡æ€»å¥–åŠ±: {aggregated['avg_sum_reward']:.4f}")
            print(f"  æˆåŠŸç‡: {aggregated['pc_success']:.2f}%")
            print(f"  è¯„ä¼°æ—¶é—´: {aggregated['eval_s']:.2f} ç§’")
            
            # æ£€æŸ¥è¾“å‡ºæ–‡ä»¶
            eval_info_file = Path(output_dir) / "eval_info.json"
            if eval_info_file.exists():
                print(f"âœ… è¯„ä¼°ä¿¡æ¯æ–‡ä»¶å·²ç”Ÿæˆ: {eval_info_file}")
            
            videos_dir = Path(output_dir) / "videos"
            if videos_dir.exists():
                video_files = list(videos_dir.glob("*.mp4"))
                print(f"âœ… ç”Ÿæˆè§†é¢‘æ–‡ä»¶: {len(video_files)} ä¸ª")
        
        return True
        
    except Exception as e:
        print(f"âŒ ç›´æ¥è¯„ä¼°æµ‹è¯•å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
        return False

def test_task_model():
    """æµ‹è¯•è¯„ä¼°ä»»åŠ¡æ¨¡å‹"""
    print("\nğŸ§ª æµ‹è¯•è¯„ä¼°ä»»åŠ¡æ¨¡å‹...")
    
    try:
        # åˆ›å»ºè¯„ä¼°ä»»åŠ¡
        eval_task = EvaluationTask(
            task_id=999,
            user_id=1,
            model_uuid="test_model_uuid",
            model_type="act",
            env_config={"type": "aloha", "task": "AlohaInsertion-v0"},
            eval_config={"n_episodes": 5, "batch_size": 2},
            seed=1000,
            max_episodes_rendered=3,
            return_episode_data=False
        )
        
        print(f"âœ… è¯„ä¼°ä»»åŠ¡åˆ›å»ºæˆåŠŸ:")
        print(f"  ä»»åŠ¡ ID: {eval_task.task_id}")
        print(f"  ç”¨æˆ· ID: {eval_task.user_id}")
        print(f"  æ¨¡å‹ UUID: {eval_task.model_uuid}")
        print(f"  æ¨¡å‹ç±»å‹: {eval_task.model_type}")
        print(f"  çŠ¶æ€: {eval_task.status}")
        
        # éªŒè¯é»˜è®¤å€¼
        assert eval_task.seed == 1000
        assert eval_task.max_episodes_rendered == 3
        assert eval_task.return_episode_data == False
        assert eval_task.status == "pending"
        assert eval_task.eval_results is None
        
        print(f"âœ… è¯„ä¼°ä»»åŠ¡æ¨¡å‹éªŒè¯é€šè¿‡")
        return True
        
    except Exception as e:
        print(f"âŒ è¯„ä¼°ä»»åŠ¡æ¨¡å‹æµ‹è¯•å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
        return False

async def test_minio_connection():
    """æµ‹è¯• MinIO è¿æ¥"""
    print("\nğŸ§ª æµ‹è¯• MinIO è¿æ¥...")
    
    try:
        from training_platform.common.minio_utils import get_minio_client
        
        # è·å– MinIO å®¢æˆ·ç«¯
        client = await get_minio_client()
        
        if client:
            print("âœ… MinIO å®¢æˆ·ç«¯è¿æ¥æˆåŠŸ")
            
            # æµ‹è¯•åˆ—å‡ºå­˜å‚¨æ¡¶
            buckets = await client.list_buckets()
            print(f"ğŸ“¦ å¯ç”¨å­˜å‚¨æ¡¶: {[bucket.name for bucket in buckets]}")
            
            # æ£€æŸ¥é»˜è®¤å­˜å‚¨æ¡¶
            bucket_exists = await client.bucket_exists(settings.MINIO_BUCKET)
            print(f"ğŸ“¦ é»˜è®¤å­˜å‚¨æ¡¶ '{settings.MINIO_BUCKET}' å­˜åœ¨: {bucket_exists}")
            
            return True
        else:
            print("âŒ MinIO å®¢æˆ·ç«¯è¿æ¥å¤±è´¥")
            return False
            
    except Exception as e:
        print(f"âŒ MinIO è¿æ¥æµ‹è¯•å¤±è´¥: {e}")
        print("ğŸ’¡ è¿™å¯èƒ½æ˜¯å› ä¸º MinIO æœåŠ¡æœªè¿è¡Œæˆ–é…ç½®ä¸æ­£ç¡®")
        return False

async def run_comprehensive_test():
    """è¿è¡Œç»¼åˆæµ‹è¯•"""
    print("ğŸš€ å¼€å§‹ EvaluatorActor ç»¼åˆæµ‹è¯•")
    print("=" * 60)
    
    # è®¾ç½®æµ‹è¯•ç¯å¢ƒ
    test_dir, outputs_dir, video_outputs_dir = setup_test_environment()
    
    tests = [
        ("è¯„ä¼°ä»»åŠ¡æ¨¡å‹", test_task_model),
        ("MinIO è¿æ¥", test_minio_connection),
        ("ç›´æ¥è¯„ä¼°è°ƒç”¨", lambda: test_direct_evaluation(video_outputs_dir)),
        ("Hugging Face æ¨¡å‹è¯„ä¼°", lambda: test_hf_model_evaluation(video_outputs_dir)),
    ]
    
    passed = 0
    total = len(tests)
    
    for test_name, test_func in tests:
        print(f"\n{'='*20} {test_name} {'='*20}")
        
        try:
            if asyncio.iscoroutinefunction(test_func):
                result = await test_func()
            else:
                result = test_func()
                
            if result:
                passed += 1
                print(f"âœ… {test_name} æµ‹è¯•é€šè¿‡")
            else:
                print(f"âŒ {test_name} æµ‹è¯•å¤±è´¥")
        except Exception as e:
            print(f"âŒ {test_name} æµ‹è¯•å¼‚å¸¸: {e}")
    
    print(f"\n{'='*60}")
    print(f"æµ‹è¯•ç»“æœ: {passed}/{total} é€šè¿‡")
    print(f"{'='*60}")
    
    # æ¸…ç†æµ‹è¯•ç¯å¢ƒ
    cleanup_test_environment(test_dir)
    
    if passed == total:
        print("ğŸ‰ æ‰€æœ‰æµ‹è¯•é€šè¿‡ï¼EvaluatorActor å·¥ä½œæ­£å¸¸ã€‚")
        print("\nğŸ“‹ æµ‹è¯•æ€»ç»“:")
        print("âœ… è¯„ä¼°ä»»åŠ¡æ¨¡å‹åˆ›å»ºå’ŒéªŒè¯æ­£å¸¸")
        print("âœ… MinIO è¿æ¥åŠŸèƒ½æ­£å¸¸ï¼ˆå¦‚æœé…ç½®äº†ï¼‰")
        print("âœ… ç›´æ¥è¯„ä¼°é€»è¾‘å·¥ä½œæ­£å¸¸")
        print("âœ… Actor æ¨¡å¼è¯„ä¼°å·¥ä½œæ­£å¸¸")
        print("âœ… Hugging Face æ¨¡å‹åŠ è½½å’Œè¯„ä¼°æ­£å¸¸")
        
        print("\nğŸ¯ EvaluatorActor åŠŸèƒ½éªŒè¯:")
        print("â€¢ å¯ä»¥æˆåŠŸåŠ è½½é¢„è®­ç»ƒæ¨¡å‹")
        print("â€¢ è¯„ä¼°æµç¨‹å®Œæ•´ï¼Œç»“æœå‡†ç¡®")
        print("â€¢ Actor ç”Ÿå‘½å‘¨æœŸç®¡ç†æ­£å¸¸")
        print("â€¢ é”™è¯¯å¤„ç†å’ŒçŠ¶æ€æŠ¥å‘Šæ­£å¸¸")
        print("â€¢ å¯ä»¥ç”¨äºç”Ÿäº§ç¯å¢ƒçš„æ¨¡å‹è¯„ä¼°")
        
    else:
        print("âš ï¸  éƒ¨åˆ†æµ‹è¯•å¤±è´¥ï¼Œè¯·æ£€æŸ¥ç›¸å…³åŠŸèƒ½ã€‚")
        print("\nğŸ”§ å¯èƒ½çš„è§£å†³æ–¹æ¡ˆ:")
        print("1. ç¡®ä¿ Ray å·²æ­£ç¡®åˆå§‹åŒ–")
        print("2. æ£€æŸ¥ç½‘ç»œè¿æ¥ï¼Œç¡®ä¿èƒ½è®¿é—® Hugging Face Hub")
        print("3. ç¡®ä¿ MinIO æœåŠ¡æ­£åœ¨è¿è¡Œï¼ˆå¦‚æœæµ‹è¯• MinIO åŠŸèƒ½ï¼‰")
        print("4. æ£€æŸ¥ LeRobot ç¯å¢ƒæ˜¯å¦æ­£ç¡®å®‰è£…")
        print("5. éªŒè¯æ‰€æœ‰ä¾èµ–åŒ…æ˜¯å¦å·²å®‰è£…")
    
    return passed == total

def main():
    """ä¸»å‡½æ•°"""
    try:
        success = asyncio.run(run_comprehensive_test())
        
        if success:
            print("\nğŸŠ EvaluatorActor æµ‹è¯•å®Œæˆï¼ŒåŠŸèƒ½æ­£å¸¸ï¼")
            print("ğŸš€ å¯ä»¥å¼€å§‹ä½¿ç”¨ EvaluatorActor è¿›è¡Œæ¨¡å‹è¯„ä¼°ã€‚")
        else:
            print("\nâŒ EvaluatorActor æµ‹è¯•å¤±è´¥ï¼Œéœ€è¦è¿›ä¸€æ­¥è°ƒè¯•ã€‚")
        
        return 0 if success else 1
        
    except KeyboardInterrupt:
        print("\nâ¹ï¸  æµ‹è¯•è¢«ç”¨æˆ·ä¸­æ–­")
        return 1
    except Exception as e:
        print(f"\nğŸ’¥ æµ‹è¯•è¿è¡Œå¼‚å¸¸: {e}")
        import traceback
        traceback.print_exc()
        return 1
    finally:
        # ç¡®ä¿ Ray æ¸…ç†
        if ray.is_initialized():
            ray.shutdown()
            print("ğŸ‘‹ Ray å·²å…³é—­")

if __name__ == "__main__":
    exit_code = main()
    sys.exit(exit_code) 